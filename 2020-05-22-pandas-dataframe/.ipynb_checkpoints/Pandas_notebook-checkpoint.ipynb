{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame de pandas\n",
    "## 1.1 Complément - niveau intermédiaire\n",
    "### 1.1.1 Création d’une DataFrame\n",
    "\n",
    "Une DataFrame est un tableau numpy à deux dimensions avec un index pour les lignes et un index\n",
    "pour les colonnes. Il y a de nombreuses manières de construire une DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        age  height\n",
      "alice  30.0   150.0\n",
      "bob    20.0     NaN\n",
      "julie  50.0   168.0\n",
      "marc    NaN   170.0\n"
     ]
    }
   ],
   "source": [
    "# Regardons la construction d'une DataFrame\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Créons une Series pour définir des âges\n",
    "age = pd.Series([30, 20, 50], index=['alice', 'bob', 'julie'])\n",
    "\n",
    "# et une Series pour définir des tailles\n",
    "height = pd.Series([150, 170, 168], index=['alice', 'marc', 'julie'])\n",
    "\n",
    "# On peut maintenant combiner ces deux Series en DataFrame,\n",
    "# chaque Series définissant une colonne, une manière de le faire est\n",
    "# de définir un dictionnaire qui contient pour clef le nom de la colonne\n",
    "# et pour valeur la Series correspondante\n",
    "stat = pd.DataFrame({'age': age, 'height': height})\n",
    "print(stat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que pandas fait automatiquement l’alignement des index, lorsqu’une valeur n’est pas\n",
    "présente, elle est automatiquement remplacée par NaN. Panda va également broadcaster une valeur\n",
    "unique définissant une colonne sur toutes les lignes. Regardons cela :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        age  height  city\n",
      "alice  30.0   150.0  Nice\n",
      "bob    20.0     NaN  Nice\n",
      "julie  50.0   168.0  Nice\n",
      "marc    NaN   170.0  Nice\n"
     ]
    }
   ],
   "source": [
    "stat = pd.DataFrame({'age': age, 'height': height, 'city': 'Nice'})\n",
    "print(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['alice', 'bob', 'julie', 'marc'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    " # On peut maintenant accéder aux index des lignes et des colonnes\n",
    "# l'index des lignes\n",
    "print(stat.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'height', 'city'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# l'index des colonnes\n",
    "print(stat.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a de nombreuses manières d’accéder aux éléments de la DataFrame, certaines sont bonnes et\n",
    "d’autres à proscrire, commençons par prendre de bonnes habitudes. Comme il s’agit d’une structure\n",
    "à deux dimensions, il faut donner un indice de ligne et de colonne :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quel est l'âge de alice\n",
    "a = stat.loc['alice', 'age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.float64, 30.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a est un flottant\n",
    "type(a), a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'âge moyen est de 33.3 ans.\n"
     ]
    }
   ],
   "source": [
    "# Quel est la moyenne de tous les âges\n",
    "c = stat.loc[:, 'age']\n",
    "m = c.mean()\n",
    "print(f\"L'âge moyen est de {m:.1f} ans.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# c est une Series\n",
    "type(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# et m est un flottant\n",
    "type(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut déjà noter plusieurs choses intéressantes :\n",
    "* On peut utiliser .loc[] et .iloc comme pour les Series. Pour les DataFrame c’est encore\n",
    "plus important parce qu’il y a plus de risques d’ambiguïtés (notamment entre les lignes et les\n",
    "colonnes, on y reviendra) ;\n",
    "* la méthode mean calcule la moyenne, ça n’est pas surprenant, mais ignore les NaN. C’est en\n",
    "général ce que l’on veut. Si vous vous demandez comment savoir si la méthode que vous\n",
    "utilisez ignore ou pas les NaN, le mieux est de regarder l’aide de cette méthode. Il existe pour\n",
    "un certain nombre de méthodes deux versions : une qui ignore les NaN et une autre qui les\n",
    "prend en compte ; on en reparlera.\n",
    "\n",
    "Une autre manière de construire une DataFrame est de partir d’un array de numpy, et de spécifier\n",
    "les index pour les lignes et les colonnes avec les arguments index et columns :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    x   y   z\n",
      "a   2  18  10\n",
      "b  13  17  10\n",
      "c   4   6   8\n",
      "[[ 2 18 10]\n",
      " [13 17 10]\n",
      " [ 4  6  8]]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randint(1, 20, 9).reshape(3, 3)\n",
    "p = pd.DataFrame(a, index=['a', 'b', 'c'], columns=['x', 'y', 'z'])\n",
    "print(p)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Importation et exportation de données\n",
    "En pratique, il est très fréquent que les données qu’on manipule soient stockées dans un fichier ou\n",
    "une base de données. Il existe en pandas de nombreux utilitaires pour importer et exporter des\n",
    "données et les convertir automatiquement en DataFrame. Vous pouvez importer ou exporter du\n",
    "CSV, JSON, HTML, Excel, HDF5, SQL, Python pickle, etc.\n",
    "À titre d’illustration écrivons la DataFrame p dans différents formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",x,y,z\r\n",
      "a,2,18,10\r\n",
      "b,13,17,10\r\n",
      "c,4,6,8\r\n"
     ]
    }
   ],
   "source": [
    "# écrivons notre DataFrame dans un fichier CSV\n",
    "p.to_csv('my_data.csv')\n",
    "!cat my_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"x\":{\"a\":2,\"b\":13,\"c\":4},\"y\":{\"a\":18,\"b\":17,\"c\":6},\"z\":{\"a\":10,\"b\":10,\"c\":8}}"
     ]
    }
   ],
   "source": [
    "# et dans un fichier JSON\n",
    "p.to_json('my_data.json')\n",
    "!cat my_data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on peut maintenant recharger notre fichier\n",
    "# la conversion en DataFrame est automatique\n",
    "new_p = pd.read_json('my_data.json')\n",
    "print(new_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la gestion des autres formats, comme il s’agit de quelque chose de très spécifique et sans\n",
    "difficulté particulière, je vous renvoie simplement à la documentation :\n",
    "http://pandas.pydata.org/pandas-docs/stable/io.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Manipulation d’une DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construisons maintenant une DataFrame jouet\n",
    "# voici une liste de prénoms\n",
    "names = ['alice', 'bob', 'marc', 'bill', 'sonia']\n",
    "\n",
    "# créons trois Series qui formeront les trois colonnes\n",
    "age = pd.Series([12, 13, 16, 11, 16], index=names)\n",
    "height = pd.Series([130, 140, 176, 120, 165], index=names)\n",
    "sex = pd.Series(list('fmmmf'), index=names)\n",
    "\n",
    "# créons maintenant la DataFrame\n",
    "p = pd.DataFrame({'age': age, 'height': height, 'sex': sex})\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# et chargeons le jeux de données sur les pourboires de seaborn\n",
    "import seaborn as sns\n",
    "tips = sns.load_dataset('tips')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas offre de nombreuses possibilités d’explorer les données. Attention, dans mes exemples je\n",
    "vais alterner entre le DataFrame p et le DataFrame tips suivant les besoins de l’explication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# afficher les premières lignes\n",
    "tips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# et les dernière lignes\n",
    "tips.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l'index des lignes\n",
    "p.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# et l'index des colonnes\n",
    "p.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# et afficher uniquement les valeurs\n",
    "p.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# échanger lignes et colonnes\n",
    "# cf. la transposition de matrices\n",
    "p.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour finir, il y a la méthodes describe qui permet d’obtenir des premières statistiques sur un\n",
    "DataFrame. describe permet de calculer des statistiques sur des type numériques, mais aussi sur\n",
    "des types chaînes de caractères."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# par défaut describe ne prend en compte que les colonnes numériques\n",
    "p.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mais on peut le forcer à prendre en compte toutes les colonnes\n",
    "p.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 Requêtes sur une DataFrame\n",
    "On peut maintenant commencer à faire des requêtes sur les DataFrames. Les DataFrame supportent\n",
    "la notion de masque que l’on a vue pour les ndarray de numpy et pour les Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p.loc prend soit un label de ligne\n",
    "print(p.loc['sonia'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ou alors un label de ligne ET de colonne\n",
    "print(p.loc['sonia', 'age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut mettre à la place d’une label :\n",
    "* une liste de labels ;\n",
    "* un slice sur les labels ;\n",
    "* un masque (c’est-à-dire un tableau de booléens) ;\n",
    "* un callable qui retourne une des trois premières possibilités.\n",
    "\n",
    "Noter que l’on peut également utiliser la notation .iloc[] avec les mêmes règles, mais elle est\n",
    "moins utile.\n",
    "Il est recommandé de toujours utiliser la notation .loc[lignes, colonnes] pour éviter toute ambiguïté. Les notations .loc[lignes] ou pire seulement [label] sont sources d’erreurs.\n",
    "Regardons maintenant d’autres exemples plus complexes :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# un masque sur les femmes\n",
    "p.loc[:, 'sex'] == 'f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# si bien que pour construire un tableau\n",
    "# avec uniquement les femmes\n",
    "p.loc[p.loc[:, 'sex'] == 'f', :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# si on veut ne garder uniquement\n",
    "# que les femmes de plus de 14 ans\n",
    "p.loc[(p.loc[:, 'sex'] == 'f') & (p.loc[:, 'age'] > 14), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quelle est la moyenne de 'total_bill' pour les femmes\n",
    "addition_f = tips.loc[tips.loc[:, 'sex'] == 'Female', 'total_bill'].mean()\n",
    "print(f\"addition moyenne des femmes : {addition_f:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quelle est la note moyenne des hommes\n",
    "addition_h = tips.loc[tips.loc[:, 'sex'] == 'Male', 'total_bill'].mean()\n",
    "print(f\"addition moyenne des hommes : {addition_h:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qui laisse le plus grand pourcentage de pourboire :\n",
    "# les hommes ou les femmes ?\n",
    "\n",
    "pourboire_f = tips.loc[tips.loc[:, 'sex'] == 'Female', 'tip'].mean()\n",
    "pourboire_h = tips.loc[tips.loc[:, 'sex'] == 'Male', 'tip'].mean()\n",
    "\n",
    "print(f\"Les femmes laissent {pourboire_f/addition_f:.2%} de pourboire\")\n",
    "print(f\"Les hommes laissent {pourboire_h/addition_h:.2%} de pourboire\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.5 Erreurs fréquentes et ambiguïtés sur les requêtes\n",
    "Nous avons vu une manière simple et non ambiguë de faire des requêtes sur les DataFrame. Nous allons voir qu’il existe d’autres manières qui ont pour seul avantage d’être plus concises, mais sources de nombreuses erreurs.\n",
    "\n",
    "Prévilégiez toujours la notation .loc[lignes, colonnes] sinon, soyez sûr de savoir ce qui est réellement calculé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commençons par la notation la plus classique\n",
    "p['sex'] # prend forcément un label de colonne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mais par contre, si on passe un slice, c'est forcément des lignes,\n",
    "# assez perturbant et source de confusion.\n",
    "p['alice': 'marc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on peut même directement accéder à une colonne par son nom\n",
    "p.age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais c’est fortement déconseillé parce que si un attribut de même nom existe sur une DataFrame,\n",
    "alors la priorité est donnée à l’attribut, et non à la colonne :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajoutons une colonne qui a pour nom une méthode qui existe sur\n",
    "# les DataFrame\n",
    "p['mean'] = 1\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# je peux bien accéder\n",
    "# à la colonne sex\n",
    "p.sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mais pas à la colonne mean\n",
    "p.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# à nouveau, la seule méthode non ambiguë est d'utiliser .loc\n",
    "p.loc[:, 'mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supprimons maintenant la colonne mean *en place* (par défaut,\n",
    "# drop retourne une nouvelle DataFrame)\n",
    "p.drop(columns='mean', inplace=True)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour aller plus loin, vous pouvez lire la documentation officielle :\n",
    "http://pandas.pydata.org/pandas-docs/stable/indexing.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.6 Universal functions et pandas\n",
    "\n",
    "Ça n’est pas une surprise, les Series et DataFrame de pandas supportent les ufunc de numpy. Mais il y a une subtilité. Il est parfaitement légitime et correct d’appliquer une ufunc de numpy sur les éléments d’une DataFrame :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.DataFrame(np.random.randint(\n",
    "1, 10, 9).reshape(3, 3), columns=list('abc'))\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous remarquons que comme on s’y attend, la ufunc a été appliquée à chaque élément de la\n",
    "DataFrame et que les labels des lignes et colonnes ont été préservés.\n",
    "Par contre, si l’on a besoin d’alignement de labels, c’est le cas avec toutes les opérations qui s’appliquent sur deux objets comme une addition, alors les ufunc de numpy ne vont pas faire ce à quoi on s’attend. Elles vont faire les opérations sur les tableaux numpy sans prendre en compte les labels.\n",
    "Pour avoir un alignement des labels, il faut utiliser les ufunc de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prenons deux Series\n",
    "s1 = pd.Series([10, 20, 30],\n",
    "index=list('abc'))\n",
    "print(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "s2 = pd.Series([12, 22, 32],\n",
    "index=list('acd'))\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la ufunc numpy fait la somme\n",
    "# des arrays sans prendre en compte\n",
    "# les labels, donc sans alignement\n",
    "np.add(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la ufunc pandas va faire\n",
    "# un alignement des labels\n",
    "# cet appel est équivalent à s1 + s2\n",
    "s1.add(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comme on l'a vu sur le complément précédent, les valeurs absentes sont\n",
    "# remplacées par NaN, mais on peut changer ce comportement lors de\n",
    "# l'appel de .add\n",
    "s1.add(s2, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regardons un autre exemple sur des DataFrame\n",
    "# on affiche tout ça dans les cellules suivantes\n",
    "names = ['alice', 'bob', 'charle']\n",
    "\n",
    "bananas = pd.Series([10, 3, 9], index=names)\n",
    "oranges = pd.Series([3, 11, 6], index=names)\n",
    "fruits_jan = pd.DataFrame({'bananas': bananas, 'orange': oranges})\n",
    "\n",
    "bananas = pd.Series([6, 1], index=names[:-1])\n",
    "apples = pd.Series([8, 5], index=names[1:])\n",
    "fruits_feb = pd.DataFrame({'bananas': bananas, 'apples': apples})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ce qui donne\n",
    "fruits_jan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# et\n",
    "fruits_feb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regardons maintenant la somme des fruits mangés\n",
    "eaten_fruits = fruits_jan + fruits_feb\n",
    "print(eaten_fruits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On a bien un alignement des labels, mais il y a beaucoup de valeurs\n",
    "# manquantes. Corrigeons cela on remplaçant les valeurs manquantes par 0\n",
    "eaten_fruits = fruits_jan.add(fruits_feb, fill_value=0)\n",
    "print(eaten_fruits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notons que lorsqu’une valeur est absente dans toutes les DataFrame, NaN est conservé.\n",
    "Un dernière subtilité à connaître lors de l’alignement des labels intervient lorsque vous faites une opération sur une DataFrame et une Series. pandas va considérer la Series comme une ligne et va la broadcaster sur les autres lignes. Par conséquent, l’index de la Series va être considéré comme des colonnes et aligné avec les colonnes de la DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(\n",
    "    np.random.randint(1, 10, size=(3, 3)),\n",
    "    columns=list('abc'), index=list('xyz'))\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_row = pd.Series(\n",
    "    [100, 200, 300],\n",
    "    index=list('abc'))\n",
    "series_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_col = pd.Series(\n",
    "    [400, 500, 600],\n",
    "    index=list('xyz'))\n",
    "series_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la Series est considérée comme une ligne et son index\n",
    "# s'aligne sur les colonnes de la DataFrame\n",
    "# la Series va être broadcastée\n",
    "# sur les autres lignes de la DataFrame\n",
    "\n",
    "dataframe + series_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# du coup si les labels ne correspondent pas,\n",
    "# le résultat sera le suivant\n",
    "\n",
    "dataframe + series_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on peut dans ce cas, changer le comportement par défaut en forçant\n",
    "# l'alignement de la Series suivant un autre axe avec l'argument axis\n",
    "\n",
    "dataframe.add(series_col, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, axis=0 signifie que la Series est considérée comme une colonne est qu’elle va être broadcastée sur les autres colonnes (le long de l’axe de ligne)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.7 Opérations sur les chaînes de caractères\n",
    "Nous allons maintenant parler de la vectorisation des opérations sur les chaînes de caractères. Il y\n",
    "a plusieurs choses importantes à savoir :\n",
    "* les méthodes sur les chaînes de caractères ne sont disponibles que pour les Series et les\n",
    "Index, mais pas pour les DataFrame ;\n",
    "* ces méthodes ignorent les NaN et remplacent les valeurs qui ne sont pas des chaînes de carac-\n",
    "tères par NaN ;\n",
    "* ces méthodes retournent une copie de l’objet (Series ou Index), il n’y a pas de modification\n",
    "en place ;\n",
    "* la plupart des méthodes Python sur le type str existe sous forme vectorisée ;\n",
    "* on accède à ces méthodes avec la syntaxe :\n",
    "    – Series.str.<vectorized method name>\n",
    "    – Index.str.<vectorized method name>\n",
    "Regardons quelques exemples :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créons une Series avec des noms ayant une capitalisation inconsistante\n",
    "# et une mauvaise gestion des espaces\n",
    "\n",
    "names = ['alice ', ' bOB', 'Marc', 'bill', 3, ' JULIE ', np.NaN]\n",
    "age = pd.Series(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nettoyons maintenant ces données\n",
    "\n",
    "# on met en minuscule\n",
    "a = age.str.lower()\n",
    "\n",
    "# on enlève les espaces\n",
    "a = a.str.strip()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comme les méthodes vectorisées retournent un objet de même type, on\n",
    "# peut les chaîner comme ceci\n",
    "\n",
    "[x for x in age.str.lower().str.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut également utiliser l’indexation des str de manière vectorisée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.str[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour aller plus loin vous pouvez lire la documentation officielle :\n",
    "http://pandas.pydata.org/pandas-docs/stable/text.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.8 Gestion des valeurs manquantes\n",
    "Nous avons vu que des opérations sur les DataFrame pouvaient générer des valeurs NaN lors de\n",
    "l’alignement. Il est également possible d’avoir de telles valeurs manquantes dans votre jeu de don-\n",
    "nées original. pandas offre plusieurs possibilités pour gérer correctement ces valeurs manquantes.\n",
    "Avant de voir ces différentes possibilités, définissons cette notion de valeur manquante.\n",
    "Une valeur manquante peut-être représentée avec pandas soit par np.NaN soit par l’objet Python\n",
    "None.\n",
    "* np.NaN est un objet de type float, par conséquent il ne peut apparaître que dans un array de\n",
    "float ou un array d’object. Notons que np.NaN apparaît avec pandas comme simplement\n",
    "NaN et que dans la suite on utilise de manière indifférente les deux notations, par contre, dans\n",
    "du code, il faut obligatoirement utiliser np.NaN ;\n",
    "– si on ajoute un NaN dans un array d’entier, ils seront convertis en float64 ;\n",
    "– si on ajoute un NaN dans un array de booléens, ils seront convertis en object ;\n",
    "* NaN est contaminant, toute opération avec un NaN a pour résultat NaN ;\n",
    "* lorsque l’on utilise None, il est automatiquement converti en NaN lorsque le type de l’array est numérique.\n",
    "\n",
    "Illustrons ces propriétés :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# une Series d'entiers\n",
    "s = pd.Series([1, 2])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on insère un NaN, la Series est alors convertie en float64\n",
    "s[0] = np.NaN\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on réinitialise\n",
    "s = pd.Series([1, 2])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# et on insère None\n",
    "s[0] = None\n",
    "\n",
    "# Le résultat est le même\n",
    "# None est converti en NaN\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardons maintenant, les méthodes de pandas pour gérer les valeurs manquantes (donc NaN ou None) :\n",
    "* isna() retourne un masque mettant à True les valeurs manquantes (il y a un alias isnull()) ;\n",
    "* notna() retourne un masque mettant à False les valeurs manquantes (il y a un alias notnull()) ;\n",
    "* dropna() retourne un nouvel objet sans les valeurs manquantes ;\n",
    "* fillna() retourne un nouvel objet avec les valeurs manquantes remplacées.\n",
    "On remarque que l’ajout d’alias pour les méthodes est de nouveau une source de confusion avec\n",
    "laquelle il faut vivre.\n",
    "On remarque également qu’alors que isnull() et notnull() sont des méthodes simples, dropna()\n",
    "et fillna() impliquent l’utilisation de stratégies. Regardons cela :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# créons une DataFrame avec quelques valeurs manquantes\n",
    "names = ['alice', 'bob', 'charles']\n",
    "bananas = pd.Series([6, 1], index=names[:-1])\n",
    "apples = pd.Series([8, 5], index=names[1:])\n",
    "fruits_feb = pd.DataFrame({'bananas': bananas, 'apples': apples})\n",
    "print(fruits_feb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits_feb.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruits_feb.notna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par défaut, dropna() va enlever toutes les lignes qui contiennent au moins une valeur manquante.\n",
    "Mais on peut changer ce comportement avec des arguments :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pd.DataFrame([[1, 2, np.NaN], [3, np.NaN, np.NaN], [7, 5, np.NaN]])\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comportement par défaut, j'enlève toutes les lignes avec au moins\n",
    "# une valeur manquante; il ne reste rien !\n",
    "p.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maintenant, je fais l'opération par colonne\n",
    "p.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# je fais l'opération par colonne si toute la colonne est manquante\n",
    "p.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# je fais l'opération par ligne si au moins 2 valeurs sont manquantes\n",
    "p.dropna(thresh=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par défaut, fillna() remplace les valeurs manquantes avec un argument pas défaut. Mais on peut ici aussi changer ce comportement. Regardons cela :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# je remplace les valeurs manquantes par -1\n",
    "p.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# je remplace les valeurs manquantes avec la valeur suivante sur la colonne\n",
    "# bfill est pour back fill, c'est-à-dire remplace en arrière à partir des\n",
    "# valeurs existantes\n",
    "p.fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# je remplace les valeurs manquantes avec la valeur précédente sur la ligne\n",
    "# ffill est pour forward fill, remplace en avant à partir des valeurs\n",
    "# existantes\n",
    "p.fillna(method='ffill', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardez l’aide de ces méthodes pour aller plus loin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.dropna?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.fillna?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.9 Analyse statistique des données\n",
    "Nous n’avons pas le temps de couvrir les possibilités d’analyse statistique de la suite data science de Python. pandas offre quelques possibilités basiques avec des calculs de moyennes, d’écarts types ou de covariances que l’on peut éventuellement appliquer par fenêtres à un jeux de données. Pour avoir plus de détails dessus vous pouvez consulter cette documentation :\n",
    "http://pandas.pydata.org/pandas-docs/stable/computation.html\n",
    "Dans la suite data science de Python, il a aussi des modules spécialisés dans l’analyse statistique\n",
    "comme :\n",
    "* StatsModels\n",
    "* ScikitLearn\n",
    "ou des outils de calculs scientifiques plus génériques comme SciPy.\n",
    "De nouveau, il s’agit d’outils appliqués à des domaines spécifiques et ils se basent tous sur le couple\n",
    "numpy/pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Complément - niveau avancé\n",
    "## 1.2.1 Les MultiIndex\n",
    "pandas avait historiquement d’autres structures de données en plus des Series et des DataFrame permettant d’exprimer des dimensionnalités supérieures à 2, comme par exemple les Panel. Mais pour des raisons de maintenance du code et d’optimisation, les développeurs ont décidé de ne garder que les Series et les DataFrame. Alors, comment exprimer des données avec plus de deux dimensions ?\n",
    "On utilise pour cela des MultiIndex. Un MultiIndex est un index qui peut être utilisé partout où l’on utilise un index (dans une Series, ou comme ligne ou colonne d’une DataFrame) et qui a pour caractéristique d’avoir plusieurs niveaux.\n",
    "Comme tous types d’index, et parce qu’un MultiIndex est une sous classe d’Index, pandas va correctement aligner les Series et les DataFrame avec des MultiIndex.\n",
    "Regardons tout de suite un exemple :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construisons une DataFrame jouet\n",
    "\n",
    "# voici une liste de prénoms\n",
    "names = ['alice', 'bob', 'sonia']\n",
    "\n",
    "# créons trois Series qui formeront trois colonnes\n",
    "age = pd.Series([12, 13, 16], index=names)\n",
    "height = pd.Series([130, 140, 165], index=names)\n",
    "sex = pd.Series(list('fmf'), index=names)\n",
    "\n",
    "# créons maintenant la DataFrame\n",
    "p = pd.DataFrame({'age': age, 'height': height, 'sex': sex})\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unstack, en première approximation, permet de passer d'une DataFrame à\n",
    "# une Series avec un MultiIndex\n",
    "s = p.unstack()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# et voici donc l'index de cette Series\n",
    "s.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe évidemment des moyens de créer directement un MultiIndex et ensuite de le définir comme index d’une Series ou comme index de ligne ou colonne d’une DataFrame :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on peut créer un MultiIndex à partir d'une liste de liste\n",
    "names = ['alice', 'alice', 'alice', 'bob', 'bob', 'bob']\n",
    "age = [2014, 2015, 2016, 2014, 2015, 2016]\n",
    "s_list = pd.Series([40, 42, 45, 38, 40, 40], index=[names, age])\n",
    "print(s_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ou à partir d'un dictionnaire de tuples\n",
    "s_tuple = pd.Series({('alice', 2014): 40,\n",
    "    ('alice', 2015): 42,\n",
    "    ('alice', 2016): 45,\n",
    "    ('bob', 2014): 38,\n",
    "    ('bob', 2015): 40,\n",
    "    ('bob', 2016): 40})\n",
    "print(s_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ou avec la méthode from_product()\n",
    "name = ['alice', 'bob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = [2014, 2015, 2016]\n",
    "i = pd.MultiIndex.from_product([name, year])\n",
    "s = pd.Series([40, 42, 45, 38, 40, 40], index=i)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut même nommer les niveaux d’un MultiIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ['alice', 'bob']\n",
    "year = [2014, 2015, 2016]\n",
    "i = pd.MultiIndex.from_product([name, year], names=['name', 'year'])\n",
    "s = pd.Series([40, 42, 45, 38, 40, 40], index=i)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on peut changer le nom des niveaux du MultiIndex\n",
    "s.index.names = ['NAMES', 'YEARS']\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créons maintenant une DataFrame jouet avec des MultiIndex pour étudier comment accéder aux éléments de la DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pd.MultiIndex.from_product([[2013, 2014],\n",
    "        [1, 2, 3]],\n",
    "        names=['year',\n",
    "            'visit'])\n",
    "columns = pd.MultiIndex.from_product([['Bob', 'Sue'],\n",
    "        ['avant', 'arrière']],\n",
    "        names=['client',\n",
    "            'pression'])\n",
    "\n",
    "# on crée des pressions de pneus factices\n",
    "data = 2 + np.random.rand(6, 4)\n",
    "\n",
    "# on crée la DataFrame\n",
    "mecanics_data = pd.DataFrame(data, index=index, columns=columns)\n",
    "print(mecanics_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y a plusieurs manières d’accéder aux éléments, mais une seule que l’on recommande :\n",
    "utilisez la notation .loc[ligne, colonne], .iloc[ligne, colonne]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pression en 2013 pour Bob\n",
    "mecanics_data.loc[2013, 'Bob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour accéder aux sous niveaux du MultiIndex, on utilise des tuples\n",
    "mecanics_data.loc[(2013, 2), ('Bob', 'avant')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le slice sur le MultiIndex est un peu délicat. On peut utiliser la notation : si on veut slicer sur tous les éléments d’un MultiIndex, sans prendre en compte un niveau. Si on spécifie les niveaux, il faut utiliser un objet slice ou pd.IndexSlice :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice(None) signifie tous les éléments du niveau\n",
    "print(mecanics_data.loc[slice((2013, 2), (2014, 1)), ('Sue', slice(None))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on peut utiliser la notation : si on ne distingue par les niveaux\n",
    "print(mecanics_data.loc[(slice(None), slice(1, 2)), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on peut aussi utiliser pd.IndexSlice pour slicer avec une notation\n",
    "# un peu plus concise\n",
    "idx = pd.IndexSlice\n",
    "print(mecanics_data.loc[idx[:, 1:2], idx['Sue', :]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour aller plus loin, regardez la documentation des MultiIndex :\n",
    "http://pandas.pydata.org/pandas-docs/stable/advanced.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Conclusion\n",
    "La DataFrame est la structure de données la plus souple et la plus puissante de pandas. Nous avons\n",
    "vu comment créer des DataFrame et comment accéder aux éléments. Nous verrons dans le prochain\n",
    "complément les techniques permettant de faire des opérations complexes (et proches dans l’esprit\n",
    "de ce que l’on peut faire avec une base de données) comme les opérations de merge ou de groupby."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Opération avancées en pandas\n",
    "## 2.1 Complément - niveau intermédiaire\n",
    "### 2.1.1 Introduction\n",
    "pandas supporte des opérations de manipulation des Series et DataFrame qui sont similaires dans l’esprit à ce que l’on peut faire avec une base de données et le langage SQL, mais de manière plus intuitive et expressive et beaucoup plus efficacement puisque les opérations se déroulent toutes en mémoire.\n",
    "Vous pouvez concaténer (concat) des DataFrame, faire des jointures (merge), faire des regroupe-\n",
    "ments (groupby) ou réorganiser les index (pivot).\n",
    "Nous allons dans la suite développer ces différentes techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Concaténations avec concat\n",
    "concat est utilisé pour concaténer des Series ou des DataFrame. Regardons un exemple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.Series([30, 35], index=['alice', 'bob'])\n",
    "s2 = pd.Series([32, 22, 29], index=['bill', 'alice', 'jo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([s1, s2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque, cependant, que par défaut il n’y a pas de contrôle sur les labels d’index dupliqués.\n",
    "On peut corriger cela avec l’argument verify_integrity, qui va produire une exception s’il y a des labels d’index communs. Évidemment, cela a un coût de calcul supplémentaire, ça n’est donc à utiliser que si c’est nécessaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    pd.concat([s1, s2], verify_integrity=True)\n",
    "except ValueError as e:\n",
    "    print(f\"erreur de concaténation:\\n{e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# créons deux Series avec les index sans recouvrement\n",
    "s1 = pd.Series(range(1000), index=[chr(x) for x in range(1000)])\n",
    "s2 = pd.Series(range(1000), index=[chr(x+2000) for x in range(1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temps de concaténation avec vérification des recouvrements\n",
    "%timeit pd.concat([s1, s2], verify_integrity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temps de concaténation sans vérification des recouvrements\n",
    "%timeit pd.concat([s1, s2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par défaut, concat concatène les lignes, c’est-à-dire que s2 sera sous s1, mais on peut changer ce comportement en utilisant l’argument axis :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = pd.DataFrame(np.random.randint(1, 10, size=(2,2)),\n",
    "columns=list('ab'), index=list('xy'))\n",
    "p2 = pd.DataFrame(np.random.randint(1, 10, size=(2,2)),\n",
    "columns=list('ab'), index=list('zt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# équivalent à pd.concat([p1, p2], axis=0)\n",
    "# concaténation des lignes\n",
    "pd.concat([p1, p2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concaténation des colonnes\n",
    "pd.concat([p1, p2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardons maintenant ce cas :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([p1, p2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous remarquez que lors de la concaténation, on prend l’union des tous les labels des index de p1 et p2, il y a donc des valeurs absentes qui sont mises à NaN. On peut contrôler ce comportement de plusieurs manières comme nous allons le voir ci-dessous.\n",
    "Par défaut (ce que l’on a fait ci-dessus), join utilise la stratégie dite outer, c’est-à-dire qu’on prend\n",
    "l’union des labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on concatène les lignes, l'argument join décide quels labels on garde\n",
    "# sur l'autre axe (ici sur les colonnes).\n",
    "# si on spécifie 'inner' on prend l'intersection des labels\n",
    "# du coup il ne reste rien ..\n",
    "pd.concat([p1, p2], join='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec join_axes, on peut spécifier les labels qu’on veut garder, sous la forme d’un objet Index :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.concat([p1, p2], join_axes=[p1.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# du coup je peux choisir très finement\n",
    "# Ancienne syntax pandas :\n",
    "# pd.concat([p1, p2], join_axes=[pd.Index(['a', 'c'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercice : Trouver les syntaxes correctes ** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notons que les Series et DataFrame ont une méthode append qui est un raccourci vers concat, mais avec moins d’options.\n",
    "Pour aller plus loin, voici la documentation officielle :\n",
    "http://pandas.pydata.org/pandas-docs/stable/merging.html#concatenating-objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Jointures avec merge\n",
    "merge est dans l’esprit similaire au JOIN en SQL. L’idée est de combiner deux DataFrame en fonction d’un critère d’égalité sur des colonnes. Regardons un exemple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'employee': ['Bob', 'Lisa', 'Sue'],\n",
    "'group': ['Accounting', 'Engineering', 'HR']})\n",
    "df2 = pd.DataFrame({'employee': ['Lisa', 'Bob', 'Sue'],\n",
    "'hire_date': [2004, 2008, 2014]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On souhaite ici combiner df1 et df2 de manière à ce que les lignes contenant le même employee soient alignées. Notre critère de merge est donc l’égalité des labels sur la colonne employee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par défaut, merge fait un inner join (ou jointure interne) en utilisant comme critère de jointure les colonnes de même nom (ici employee). inner join veut dire que pour joindre deux lignes il faut que le même employee apparaisse dans les deux DataFrame.\n",
    "\n",
    "Il existe trois type de merges :\n",
    "* one-to-one, c’est celui que l’on vient de voir. C’est le merge lorqu’il n’y a pas de labels dupliqués dans les colonnes utilisées comme critère de merge ;\n",
    "* many-to-one, c’est le merge lorsque l’une des deux colonnes contient des labels dupliqués, dans ce cas, on applique la stratégie one-to-one pour chaque label dupliqué, donc les entrées dupliquées sont préservées ;\n",
    "* many-to-many, c’est la stratégie lorsqu’il y a des entrées dupliquées dans les deux colonnes. Dans ce cas, on fait un produit cartésien des lignes.\n",
    "\n",
    "D’une manière générale, gardez en tête que pandas fait essentiellement ce à quoi on s’attend.\n",
    "Regardons cela sur des exemples :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'patient': ['Bob', 'Lisa', 'Sue'],\n",
    "    'repas': ['SS', 'SS', 'SSR']})\n",
    "df2 = pd.DataFrame({'repas': ['SS', 'SSR'],\n",
    "    'explication': ['sans sel', 'sans sucre']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la colonne commune pour le merge est 'repas' et dans une des colonnes\n",
    "# (sur df1), il y a des labels dupliqués, on applique la stratégie many-to-one\n",
    "pd.merge(df1, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'patient': ['Bob', 'Lisa', 'Sue'],\n",
    "    'repas': ['SS', 'SS', 'SSR']})\n",
    "df2 = pd.DataFrame({'repas': ['SS', 'SS', 'SSR'],\n",
    "    'explication': ['sans sel', 'légumes', 'sans sucre']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la colonne commune pour le merge est 'repas' et dans les deux colonnes\n",
    "# il y a des labels dupliqués, on applique la stratégie many-to-many\n",
    "pd.merge(df1,df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un merge, on peut contrôler les colonnes à utiliser comme critère de merge. Regardons ces différents cas sur des exemples :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'employee': ['Bob', 'Lisa', 'Sue'],\n",
    "'group': ['Accounting', 'Engineering', 'HR']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({'employee': ['Lisa', 'Bob', 'Sue'],\n",
    "    'hire_date': [2004, 2008, 2014]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on décide d'utiliser la colonne 'employee' comme critère de merge\n",
    "pd.merge(df1, df2, on='employee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'employee': ['Bob', 'Lisa', 'Sue'],\n",
    "    'group': ['Accounting', 'Engineering', 'HR']})\n",
    "df2 = pd.DataFrame({'name': ['Lisa', 'Bob', 'Sue'],\n",
    "    'hire_date': [2004, 2008, 2014]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mais on peut également définir un nom de colonne différent\n",
    "# à gauche et à droite\n",
    "m = pd.merge(df1,df2, left_on='employee', right_on='name')\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dans ce cas, comme on garde les colonnes utilisées comme critère dans\n",
    "# le résultat du merge, on peut effacer la colonne inutile ainsi\n",
    "m.drop('name', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge permet également de contrôler la stratégie à appliquer lorsqu’il y a des valeurs dans une colonne utilisée comme critère de merge qui sont absentes dans l’autre colonne. C’est ce que l’on appelle jointure à gauche, jointure à droite, jointure interne (comportement par défaut) et jointure externe. Pour ceux qui ne sont pas familiers avec ces notions, regardons des exemples :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'name': ['Bob', 'Lisa', 'Sue'],\n",
    "        'pulse': [70, 63, 81]})\n",
    "df2 = pd.DataFrame({'name': ['Eric', 'Bob', 'Marc'],\n",
    "        'weight': [60, 100, 70]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la colonne 'name' est le critère de merge dans les deux DataFrame.\n",
    "# Seul Bob existe dans les deux colonnes. Dans un inner join\n",
    "# (le cas par défaut) on ne garde que les lignes pour lesquelles il y a une\n",
    "# même valeur présente à gauche et à droite\n",
    "pd.merge(df1, df2) # équivalent à pd.merge(df1, df2, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
