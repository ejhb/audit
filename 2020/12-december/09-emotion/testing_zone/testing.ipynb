{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bitbasecondac27713b0d4da4424a3036238a0ac4095",
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------IMPORT----------------------------------------------------------------------------#\n",
    "#dieu\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from time import time\n",
    "# Sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer , TfidfTransformer\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import recall_score , precision_score ,f1_score\n",
    "from sklearn import metrics , svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.naive_bayes import MultinomialNB, CategoricalNB, ComplementNB, BernoulliNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "#vis\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objs as go\n",
    "import plotly as py\n",
    "from plotly.offline import init_notebook_mode, iplot, plot\n",
    "#NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords , wordnet as wn\n",
    "from nltk import wordpunct_tokenize , WordNetLemmatizer ,sent_tokenize ,  word_tokenize\n",
    "from nltk.stem import PorterStemmer , LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------DATAFRAME DISPLAY------------------------------------------------------------------#\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.width', None)\n",
    "# pd.set_option('display.max_colwidth', -1)\n",
    "pd.reset_option(\"^display\") ##Reset display option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------FUNCTIONS--------------------------------------------------------------------#\n",
    "def get_top_n_words(corpus,d,n=None):\n",
    "    vec = CountVectorizer().fit(corpus.str(lower))\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n",
    "    if d == \"up\" :\n",
    "        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "        return words_freq[:n]\n",
    "    elif d == \"down\" :\n",
    "        words_freq=sorted(words_freq, key = lambda x: x[1], reverse=False)\n",
    "        return words_freq[:n]\n",
    "\n",
    "def tokenize(corpus):\n",
    "    #text = ''.join([ch for ch in text if ch not in dataEF[\"Text\"]])\n",
    "    tokens = nltk.word_tokenize()\n",
    "    lemma = WordNetLemmatizer()\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    # tokens = [lemmatizer.lemmatize(token) for token in tokens] \n",
    "    #OR\n",
    "    # tokens = [lemma.lemmatize(lemma.lemmatize(lemma.lemmatize(w,'v'),'n'),'a')for w in tokens]\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "def run_pipes(pipes, splits=10, test_size=0.2, seed=42):  \n",
    "    res = defaultdict(list)\n",
    "    spliter = ShuffleSplit(n_splits=splits, test_size=test_size, random_state=seed)\n",
    "    for idx_train, idx_test in spliter.split(corpus):\n",
    "        for pipe in pipes:\n",
    "            # name of the model\n",
    "            name = \"-\".join([x[0] for x in pipe.steps])\n",
    "            \n",
    "            # extract datasets\n",
    "            X_train = corpus[idx_train]\n",
    "            X_test = corpus[idx_test]\n",
    "            y_train = targets[idx_train]\n",
    "            y_test = targets[idx_test]\n",
    "            \n",
    "            # Learn\n",
    "            start = time()\n",
    "            pipe.fit(X_train, y_train)\n",
    "            fit_time = time() - start\n",
    "            \n",
    "            # predict and save results\n",
    "            y = pipe.predict(X_test)\n",
    "            res[name].append([\n",
    "                fit_time,\n",
    "                f1_score(y_test, y, average = 'micro')\n",
    "            ])\n",
    "    return res\n",
    "\n",
    "def print_table(res):\n",
    "    # Compute mean and std\n",
    "    final = {}\n",
    "    for model in res:\n",
    "        arr = np.array(res[model])\n",
    "        final[model] = {\n",
    "            \"time\" : arr[:, 0].mean().round(2),\n",
    "            \"f1_score\": [arr[:,1].mean().round(5),arr[:,1].std().round(5)]\n",
    "                    }\n",
    "\n",
    "    df = pd.DataFrame.from_dict(final, orient=\"index\").round(3)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('../data/emotion_final.csv')\n",
    "\n",
    "exclude = set(string.punctuation) # exclude = punctuation strings\n",
    "stop_word = stopwords.words('english') # we choosing stop words of english dict\n",
    "stop_word_punct = stop_sw.extend(exclude) # we add strings punctions to stop word dict\n",
    "\n",
    "corpus = np.array(df1[\"Text\"])\n",
    "targets = np.array(df1[\"Emotion\"])\n",
    "targets = np.array([1 if x == \"sadness\" else 2 if x==\"anger\" else 3 if x==\"love\" else 4 if x==\"surprise\" else 5 if x==\"fear\" else 6 for x in targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe0 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('sgd', SGDClassifier()),\n",
    "])\n",
    "pipe1 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('svm', SVC()),\n",
    "])\n",
    "pipe2 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('sgd', SGDClassifier()),\n",
    "])\n",
    "pipe3 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('svm', LinearSVC()),\n",
    "])\n",
    "pipe4 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('svm', SVC()),\n",
    "])\n",
    "pipe5 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('logit', LogisticRegression()),\n",
    "])\n",
    "pipe6 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('mult_nb', MultinomialNB()),\n",
    "])\n",
    "pipe7 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('compl_nb', ComplementNB()),\n",
    "])\n",
    "pipe8 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('bern_nb', BernoulliNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = run_pipes([pipe0, pipe1, pipe2, pipe3, pipe4, pipe5, pipe6, pipe7, pipe8], splits=5)\n",
    "print_table(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe0 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('sgd', SGDClassifier()),\n",
    "])\n",
    "pipe1 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('svm', SVC()),\n",
    "])\n",
    "pipe2 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('sgd', SGDClassifier()),\n",
    "])\n",
    "pipe3 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('svm', LinearSVC()),\n",
    "])\n",
    "pipe4 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('svm', SVC()),\n",
    "])\n",
    "pipe5 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('logit', LogisticRegression()),\n",
    "])\n",
    "pipe6 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('mult_nb', MultinomialNB()),\n",
    "])\n",
    "pipe7 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('compl_nb', ComplementNB()),\n",
    "])\n",
    "pipe8 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('bern_nb', BernoulliNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                   time        f1_score\n",
       "vect-sgd           0.64  [0.887, 0.005]\n",
       "vect-svm          91.26  [0.787, 0.003]\n",
       "vect-tfidf-sgd     0.50   [0.89, 0.003]\n",
       "vect-tfidf-svml    0.59  [0.891, 0.007]\n",
       "vect-tfidf-svm    97.59  [0.845, 0.006]\n",
       "vect-tfidf-logit   4.45  [0.854, 0.006]\n",
       "vect-mult_nb       0.25  [0.745, 0.005]\n",
       "vect-compl_nb      0.24  [0.877, 0.003]\n",
       "vect-bern_nb       0.25  [0.655, 0.008]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>f1_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>vect-sgd</th>\n      <td>0.64</td>\n      <td>[0.887, 0.005]</td>\n    </tr>\n    <tr>\n      <th>vect-svm</th>\n      <td>91.26</td>\n      <td>[0.787, 0.003]</td>\n    </tr>\n    <tr>\n      <th>vect-tfidf-sgd</th>\n      <td>0.50</td>\n      <td>[0.89, 0.003]</td>\n    </tr>\n    <tr>\n      <th>vect-tfidf-svml</th>\n      <td>0.59</td>\n      <td>[0.891, 0.007]</td>\n    </tr>\n    <tr>\n      <th>vect-tfidf-svm</th>\n      <td>97.59</td>\n      <td>[0.845, 0.006]</td>\n    </tr>\n    <tr>\n      <th>vect-tfidf-logit</th>\n      <td>4.45</td>\n      <td>[0.854, 0.006]</td>\n    </tr>\n    <tr>\n      <th>vect-mult_nb</th>\n      <td>0.25</td>\n      <td>[0.745, 0.005]</td>\n    </tr>\n    <tr>\n      <th>vect-compl_nb</th>\n      <td>0.24</td>\n      <td>[0.877, 0.003]</td>\n    </tr>\n    <tr>\n      <th>vect-bern_nb</th>\n      <td>0.25</td>\n      <td>[0.655, 0.008]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "# run base pipes\n",
    "res = run_pipes([pipe0, pipe1, pipe2, pipe3, pipe4, pipe5, pipe6, pipe7, pipe8], splits=5)\n",
    "print_table(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude = set(string.punctuation) # exclude = punctuation strings\n",
    "# stop_sw = stopwords.words('english') # we choosing stop words of english dict\n",
    "# stop_sw.extend(exclude) # we add strings punctions to stop word dict\n",
    "# lemma = WordNetLemmatizer()\n",
    "# stemmer = SnowballStemmer(\"english\") # we choosing the language english for the stemmization \n",
    "# porter = PorterStemmer() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    Emotion  Count\n0     anger   2993\n1      fear   2652\n2     happy   7029\n3      love   1641\n4   sadness   6265\n5  surprise    879\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('../data/emotion_final.csv')\n",
    "\n",
    "# print(df1['Emotion'].unique())\n",
    "df_count_emotion = df1.groupby(['Emotion']).size().reset_index(name='Count')\n",
    "print(df_count_emotion) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                    Text  Emotion  \\\n",
       "0                                i didnt feel humiliated  sadness   \n",
       "1      i can go from feeling so hopeless to so damned...  sadness   \n",
       "2       im grabbing a minute to post i feel greedy wrong    anger   \n",
       "3      i am ever feeling nostalgic about the fireplac...     love   \n",
       "4                                   i am feeling grouchy    anger   \n",
       "...                                                  ...      ...   \n",
       "21454               Melissa stared at her friend in dism     fear   \n",
       "21455  Successive state elections have seen the gover...     fear   \n",
       "21456               Vincent was irritated but not dismay     fear   \n",
       "21457  Kendall-Hume turned back to face the dismayed ...     fear   \n",
       "21458                    I am dismayed , but not surpris     fear   \n",
       "\n",
       "                                         tokenized_sents  \n",
       "0                                   [didnt, feel, humil]  \n",
       "1      [go, feel, hopeless, damn, hop, around, someon...  \n",
       "2            [im, grab, minut, post, feel, greed, wrong]  \n",
       "3      [ev, feel, nostalg, fireplac, know, stil, prop...  \n",
       "4                                         [feel, grouch]  \n",
       "...                                                  ...  \n",
       "21454                       [meliss, star, friend, dism]  \n",
       "21455  [success, stat, elect, seen, govern, part, pum...  \n",
       "21456                              [vint, irrit, dismay]  \n",
       "21457       [kendall-hum, turn, back, fac, dismay, coup]  \n",
       "21458                                 [i, dismay, surpr]  \n",
       "\n",
       "[21459 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Emotion</th>\n      <th>tokenized_sents</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>i didnt feel humiliated</td>\n      <td>sadness</td>\n      <td>[didnt, feel, humil]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>i can go from feeling so hopeless to so damned...</td>\n      <td>sadness</td>\n      <td>[go, feel, hopeless, damn, hop, around, someon...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>im grabbing a minute to post i feel greedy wrong</td>\n      <td>anger</td>\n      <td>[im, grab, minut, post, feel, greed, wrong]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>i am ever feeling nostalgic about the fireplac...</td>\n      <td>love</td>\n      <td>[ev, feel, nostalg, fireplac, know, stil, prop...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>i am feeling grouchy</td>\n      <td>anger</td>\n      <td>[feel, grouch]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>21454</th>\n      <td>Melissa stared at her friend in dism</td>\n      <td>fear</td>\n      <td>[meliss, star, friend, dism]</td>\n    </tr>\n    <tr>\n      <th>21455</th>\n      <td>Successive state elections have seen the gover...</td>\n      <td>fear</td>\n      <td>[success, stat, elect, seen, govern, part, pum...</td>\n    </tr>\n    <tr>\n      <th>21456</th>\n      <td>Vincent was irritated but not dismay</td>\n      <td>fear</td>\n      <td>[vint, irrit, dismay]</td>\n    </tr>\n    <tr>\n      <th>21457</th>\n      <td>Kendall-Hume turned back to face the dismayed ...</td>\n      <td>fear</td>\n      <td>[kendall-hum, turn, back, fac, dismay, coup]</td>\n    </tr>\n    <tr>\n      <th>21458</th>\n      <td>I am dismayed , but not surpris</td>\n      <td>fear</td>\n      <td>[i, dismay, surpr]</td>\n    </tr>\n  </tbody>\n</table>\n<p>21459 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "df1['tokenized_sents'] = df1.apply(lambda row: word_tokenize(row['Text']), axis=1) # Tokenization\n",
    "df1['tokenized_sents'] = df1['tokenized_sents'].apply(lambda x: [item for item in x if item not in stop_sw])\n",
    "df1.tokenized_sents = [[lemma.lemmatize(word) for word in each if word not in stop_sw] for each in df1.tokenized_sents] \n",
    "df1['tokenized_sents'] = df1['tokenized_sents'].apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word.\n",
    "df1['tokenized_sents'] = df1['tokenized_sents'].apply(lambda x: [porter.stem(y) for y in x]) # Stem every word.\n",
    "df1['tokenized_sents'] = df1['tokenized_sents'].apply(lambda x: [lancaster.stem(y) for y in x]) # Stem every word.\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         tweet_id   sentiment         author  \\\n",
       "0      1956967341       empty     xoshayzers   \n",
       "1      1956967666     sadness      wannamama   \n",
       "2      1956967696     sadness      coolfunky   \n",
       "3      1956967789  enthusiasm    czareaquino   \n",
       "4      1956968416     neutral      xkilljoyx   \n",
       "...           ...         ...            ...   \n",
       "39995  1753918954     neutral  showMe_Heaven   \n",
       "39996  1753919001        love       drapeaux   \n",
       "39997  1753919005        love       JenniRox   \n",
       "39998  1753919043   happiness       ipdaman1   \n",
       "39999  1753919049        love    Alpharalpha   \n",
       "\n",
       "                                                 content  \\\n",
       "0      @tiffanylue i know  i was listenin to bad habi...   \n",
       "1      Layin n bed with a headache  ughhhh...waitin o...   \n",
       "2                    Funeral ceremony...gloomy friday...   \n",
       "3                   wants to hang out with friends SOON!   \n",
       "4      @dannycastillo We want to trade with someone w...   \n",
       "...                                                  ...   \n",
       "39995                                   @JohnLloydTaylor   \n",
       "39996                     Happy Mothers Day  All my love   \n",
       "39997  Happy Mother's Day to all the mommies out ther...   \n",
       "39998  @niariley WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEE...   \n",
       "39999  @mopedronin bullet train from tokyo    the gf ...   \n",
       "\n",
       "                                         tokenized_sents  \n",
       "0      [tiffanylu, know, listenin, bad, habit, ear, s...  \n",
       "1      [layin, n, bed, headach, ughhhh, ..., waitin, ...  \n",
       "2                [fun, ceremon, ..., gloom, friday, ...]  \n",
       "3                             [want, hang, friend, soon]  \n",
       "4      [dannycastillo, we, want, trad, someon, housto...  \n",
       "...                                                  ...  \n",
       "39995                                    [johnlloydtayl]  \n",
       "39996                         [happ, moth, day, al, lov]  \n",
       "39997  [happ, moth, 's, day, momm, wom, man, long, re...  \n",
       "39998  [niariley, wassup, beaut, follow, me, peep, ou...  \n",
       "39999  [mopedronin, bullet, train, tokyo, gf, visit, ...  \n",
       "\n",
       "[40000 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>sentiment</th>\n      <th>author</th>\n      <th>content</th>\n      <th>tokenized_sents</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1956967341</td>\n      <td>empty</td>\n      <td>xoshayzers</td>\n      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n      <td>[tiffanylu, know, listenin, bad, habit, ear, s...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1956967666</td>\n      <td>sadness</td>\n      <td>wannamama</td>\n      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n      <td>[layin, n, bed, headach, ughhhh, ..., waitin, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1956967696</td>\n      <td>sadness</td>\n      <td>coolfunky</td>\n      <td>Funeral ceremony...gloomy friday...</td>\n      <td>[fun, ceremon, ..., gloom, friday, ...]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1956967789</td>\n      <td>enthusiasm</td>\n      <td>czareaquino</td>\n      <td>wants to hang out with friends SOON!</td>\n      <td>[want, hang, friend, soon]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1956968416</td>\n      <td>neutral</td>\n      <td>xkilljoyx</td>\n      <td>@dannycastillo We want to trade with someone w...</td>\n      <td>[dannycastillo, we, want, trad, someon, housto...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>39995</th>\n      <td>1753918954</td>\n      <td>neutral</td>\n      <td>showMe_Heaven</td>\n      <td>@JohnLloydTaylor</td>\n      <td>[johnlloydtayl]</td>\n    </tr>\n    <tr>\n      <th>39996</th>\n      <td>1753919001</td>\n      <td>love</td>\n      <td>drapeaux</td>\n      <td>Happy Mothers Day  All my love</td>\n      <td>[happ, moth, day, al, lov]</td>\n    </tr>\n    <tr>\n      <th>39997</th>\n      <td>1753919005</td>\n      <td>love</td>\n      <td>JenniRox</td>\n      <td>Happy Mother's Day to all the mommies out ther...</td>\n      <td>[happ, moth, 's, day, momm, wom, man, long, re...</td>\n    </tr>\n    <tr>\n      <th>39998</th>\n      <td>1753919043</td>\n      <td>happiness</td>\n      <td>ipdaman1</td>\n      <td>@niariley WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEE...</td>\n      <td>[niariley, wassup, beaut, follow, me, peep, ou...</td>\n    </tr>\n    <tr>\n      <th>39999</th>\n      <td>1753919049</td>\n      <td>love</td>\n      <td>Alpharalpha</td>\n      <td>@mopedronin bullet train from tokyo    the gf ...</td>\n      <td>[mopedronin, bullet, train, tokyo, gf, visit, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>40000 rows × 5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 124
    }
   ],
   "source": [
    "df2 = pd.read_csv('../data/text_emotion.csv')\n",
    "\n",
    "df2['tokenized_sents'] = df2.apply(lambda row: word_tokenize(row['content']), axis=1) # Tokenization\n",
    "df2['tokenized_sents'] = df2['tokenized_sents'].apply(lambda x: [item for item in x if item not in stop_sw])\n",
    "df2.tokenized_sents = [[lemma.lemmatize(word) for word in each if word not in stop_sw] for each in df2.tokenized_sents] \n",
    "df2['tokenized_sents'] = df2['tokenized_sents'].apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word.\n",
    "df2['tokenized_sents'] = df2['tokenized_sents'].apply(lambda x: [porter.stem(y) for y in x]) # Stem every word.\n",
    "df2['tokenized_sents'] = df2['tokenized_sents'].apply(lambda x: [lancaster.stem(y) for y in x]) # Stem every word.\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-128-f7d5594e370d>, line 2)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-128-f7d5594e370d>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    df2\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "df2.tokenized_sents = [[''.join(i)[0] for i in each for each in df2.tokenized_sents] \n",
    "df2\n",
    "# df2.tokenized_sents = [[''.join(i)[0]for i in [df2.tokenized_sents]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         tweet_id   sentiment         author  \\\n",
       "0      1956967341       empty     xoshayzers   \n",
       "1      1956967666     sadness      wannamama   \n",
       "2      1956967696     sadness      coolfunky   \n",
       "3      1956967789  enthusiasm    czareaquino   \n",
       "4      1956968416     neutral      xkilljoyx   \n",
       "...           ...         ...            ...   \n",
       "39995  1753918954     neutral  showMe_Heaven   \n",
       "39996  1753919001        love       drapeaux   \n",
       "39997  1753919005        love       JenniRox   \n",
       "39998  1753919043   happiness       ipdaman1   \n",
       "39999  1753919049        love    Alpharalpha   \n",
       "\n",
       "                                                 content  \\\n",
       "0      @tiffanylue i know  i was listenin to bad habi...   \n",
       "1      Layin n bed with a headache  ughhhh...waitin o...   \n",
       "2                    Funeral ceremony...gloomy friday...   \n",
       "3                   wants to hang out with friends SOON!   \n",
       "4      @dannycastillo We want to trade with someone w...   \n",
       "...                                                  ...   \n",
       "39995                                   @JohnLloydTaylor   \n",
       "39996                     Happy Mothers Day  All my love   \n",
       "39997  Happy Mother's Day to all the mommies out ther...   \n",
       "39998  @niariley WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEE...   \n",
       "39999  @mopedronin bullet train from tokyo    the gf ...   \n",
       "\n",
       "                                         tokenized_sents  \\\n",
       "0      tiffanylu know listenin bad habit ear start fr...   \n",
       "1          layin n bed headach ughhhh ... waitin cal ...   \n",
       "2                       fun ceremon ... gloom friday ...   \n",
       "3                                  want hang friend soon   \n",
       "4      dannycastillo we want trad someon houston tick...   \n",
       "...                                                  ...   \n",
       "39995                                      johnlloydtayl   \n",
       "39996                               happ moth day al lov   \n",
       "39997  happ moth 's day momm wom man long re momm som...   \n",
       "39998  niariley wassup beaut follow me peep out my ne...   \n",
       "39999  mopedronin bullet train tokyo gf visit jap sin...   \n",
       "\n",
       "                                                  simple  \n",
       "0      tiffanylu know listenin bad habit ear start fr...  \n",
       "1          layin n bed headach ughhhh ... waitin cal ...  \n",
       "2                       fun ceremon ... gloom friday ...  \n",
       "3                                  want hang friend soon  \n",
       "4      dannycastillo we want trad someon houston tick...  \n",
       "...                                                  ...  \n",
       "39995                                      johnlloydtayl  \n",
       "39996                               happ moth day al lov  \n",
       "39997  happ moth 's day momm wom man long re momm som...  \n",
       "39998  niariley wassup beaut follow me peep out my ne...  \n",
       "39999  mopedronin bullet train tokyo gf visit jap sin...  \n",
       "\n",
       "[40000 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>sentiment</th>\n      <th>author</th>\n      <th>content</th>\n      <th>tokenized_sents</th>\n      <th>simple</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1956967341</td>\n      <td>empty</td>\n      <td>xoshayzers</td>\n      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n      <td>tiffanylu know listenin bad habit ear start fr...</td>\n      <td>tiffanylu know listenin bad habit ear start fr...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1956967666</td>\n      <td>sadness</td>\n      <td>wannamama</td>\n      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n      <td>layin n bed headach ughhhh ... waitin cal ...</td>\n      <td>layin n bed headach ughhhh ... waitin cal ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1956967696</td>\n      <td>sadness</td>\n      <td>coolfunky</td>\n      <td>Funeral ceremony...gloomy friday...</td>\n      <td>fun ceremon ... gloom friday ...</td>\n      <td>fun ceremon ... gloom friday ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1956967789</td>\n      <td>enthusiasm</td>\n      <td>czareaquino</td>\n      <td>wants to hang out with friends SOON!</td>\n      <td>want hang friend soon</td>\n      <td>want hang friend soon</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1956968416</td>\n      <td>neutral</td>\n      <td>xkilljoyx</td>\n      <td>@dannycastillo We want to trade with someone w...</td>\n      <td>dannycastillo we want trad someon houston tick...</td>\n      <td>dannycastillo we want trad someon houston tick...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>39995</th>\n      <td>1753918954</td>\n      <td>neutral</td>\n      <td>showMe_Heaven</td>\n      <td>@JohnLloydTaylor</td>\n      <td>johnlloydtayl</td>\n      <td>johnlloydtayl</td>\n    </tr>\n    <tr>\n      <th>39996</th>\n      <td>1753919001</td>\n      <td>love</td>\n      <td>drapeaux</td>\n      <td>Happy Mothers Day  All my love</td>\n      <td>happ moth day al lov</td>\n      <td>happ moth day al lov</td>\n    </tr>\n    <tr>\n      <th>39997</th>\n      <td>1753919005</td>\n      <td>love</td>\n      <td>JenniRox</td>\n      <td>Happy Mother's Day to all the mommies out ther...</td>\n      <td>happ moth 's day momm wom man long re momm som...</td>\n      <td>happ moth 's day momm wom man long re momm som...</td>\n    </tr>\n    <tr>\n      <th>39998</th>\n      <td>1753919043</td>\n      <td>happiness</td>\n      <td>ipdaman1</td>\n      <td>@niariley WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEE...</td>\n      <td>niariley wassup beaut follow me peep out my ne...</td>\n      <td>niariley wassup beaut follow me peep out my ne...</td>\n    </tr>\n    <tr>\n      <th>39999</th>\n      <td>1753919049</td>\n      <td>love</td>\n      <td>Alpharalpha</td>\n      <td>@mopedronin bullet train from tokyo    the gf ...</td>\n      <td>mopedronin bullet train tokyo gf visit jap sin...</td>\n      <td>mopedronin bullet train tokyo gf visit jap sin...</td>\n    </tr>\n  </tbody>\n</table>\n<p>40000 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 123
    }
   ],
   "source": [
    "corpus = df2['tokenized_sents']\n",
    "x = corpus\n",
    "x = [[' '.join(i)][0] for i in x]\n",
    "\n",
    "df2['tokenized_sents'] = x\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-- TF-IDF vector shape : (21459, 129522)\n"
     ]
    }
   ],
   "source": [
    "corpus = np.array(df1['Text'])\n",
    "x = corpus\n",
    "\n",
    "# TF-IDF Vectorizer \n",
    "v_tf = TfidfVectorizer(stop_words=stop_sw, tokenizer=tokenize, ngram_range=(1,2))\n",
    "x_tf = v_tf.fit_transform(x)\n",
    "# TF-IDF Model\n",
    "idf_values = dict(zip(v_tf.get_feature_names(), v_tf.idf_))\n",
    "words_name = v_tf.get_feature_names()\n",
    "\n",
    "print(\"-- TF-IDF vector shape :\",x_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-- Count_Vectorizer shape : (21459, 129522)\n"
     ]
    }
   ],
   "source": [
    "corpus = np.array(df1['Text'])\n",
    "x = corpus\n",
    "\n",
    "cv = CountVectorizer(stop_words=stop_sw, tokenizer=tokenize, ngram_range=(1,2))\n",
    "x_cv = cv.fit_transform(x) ## Count Vectorizer Model\n",
    "print(\"-- Count_Vectorizer shape :\",x_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train pack with TF-IDF Vectorizer \n",
    "x_tf_train, x_tf_test, y_train, y_test = train_test_split(x_tf, y, test_size=0.20, random_state=0)\n",
    "print('-- Training pack TF-IDF:',\"\\n\")\n",
    "print('-- x_tf_train rows :',np.size(x_tf_train))\n",
    "print('-- x_tf_test rows :',np.size(x_tf_test))\n",
    "print('-- ratio in % :', round(np.size(x_tf_test)/(np.size(x_tf_train)+np.size(x_tf_test))*100,2),\"\\n\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-- TF-IDF vector shape : (40000, 245784)\n"
     ]
    }
   ],
   "source": [
    "corpus = np.array(df2['content'])\n",
    "x = corpus\n",
    "\n",
    "# TF-IDF Vectorizer \n",
    "v_tf = TfidfVectorizer(stop_words=stop_sw, tokenizer=tokenize, ngram_range=(1,2))\n",
    "x_tf = v_tf.fit_transform(x)\n",
    "# TF-IDF Model\n",
    "idf_values = dict(zip(v_tf.get_feature_names(), v_tf.idf_))\n",
    "words_name = v_tf.get_feature_names()\n",
    "\n",
    "print(\"-- TF-IDF vector shape :\",x_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['@', 'tiffanylu', 'i', 'know', 'i', 'be', 'listenin', 'to', 'bad', 'habit', 'earli', 'and', 'i', 'start', 'freakin', 'at', 'his', 'part', '=', '[']\n",
      "-- Count_Vectorizer shape : (40000, 245784)\n"
     ]
    }
   ],
   "source": [
    "corpus = np.array(df2['content'])\n",
    "x = corpus\n",
    "\n",
    "cv = CountVectorizer(stop_words=stop_sw, tokenizer=tokenize, ngram_range=(1,2))\n",
    "x_cv = cv.fit_transform(x) ## Count Vectorizer Model\n",
    "print(\"-- Count_Vectorizer shape :\",x_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = run_pipes([pipe0, pipe1, pipe2, pipe3, pipe4, pipe5, pipe6, pipe7, pipe8], splits=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     Word  Number of times\n",
       "0    feel            13973\n",
       "1     and            12721\n",
       "2      to            11835\n",
       "3     the            11808\n",
       "4      of             6824\n",
       "..    ...              ...\n",
       "95   back              540\n",
       "96    has              536\n",
       "97     go              513\n",
       "98  which              510\n",
       "99    don              509\n",
       "\n",
       "[100 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Number of times</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>feel</td>\n      <td>13973</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>and</td>\n      <td>12721</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>to</td>\n      <td>11835</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>the</td>\n      <td>11808</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>of</td>\n      <td>6824</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>back</td>\n      <td>540</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>has</td>\n      <td>536</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>go</td>\n      <td>513</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>which</td>\n      <td>510</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>don</td>\n      <td>509</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "df1 = pd.read_csv('../data/emotion_final.csv')\n",
    "\n",
    "x = df1.Text\n",
    "y = df1.Emotion\n",
    "\n",
    "freq_top = get_top_n_words(x,\"up\",100)\n",
    "\n",
    "\n",
    "df_up = pd.DataFrame(freq_top, columns =['Word','Number of times'])\n",
    "y_nbr = df_up['Number of times']\n",
    "x_word = df_up['Word']\n",
    "# df_down = pd.DataFrame(freq_down, columns =['Word','Number of times'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": "rgba(255, 87, 51, 0.5)",
          "line": {
           "color": "rgb(0,0,0)",
           "width": 2.5
          }
         },
         "name": "Le score universitaire pour le transfert de connaissances par pays",
         "text": [
          "feel",
          "and",
          "to",
          "the",
          "of",
          "that",
          "feeling",
          "my",
          "in",
          "it",
          "like",
          "was",
          "so",
          "for",
          "im",
          "but",
          "me",
          "have",
          "is",
          "with",
          "this",
          "am",
          "not",
          "about",
          "be",
          "as",
          "on",
          "you",
          "at",
          "just",
          "when",
          "or",
          "all",
          "more",
          "because",
          "do",
          "can",
          "up",
          "he",
          "really",
          "by",
          "are",
          "very",
          "been",
          "if",
          "know",
          "had",
          "out",
          "her",
          "time",
          "what",
          "myself",
          "how",
          "from",
          "they",
          "little",
          "get",
          "now",
          "will",
          "being",
          "people",
          "would",
          "them",
          "she",
          "want",
          "some",
          "one",
          "an",
          "his",
          "him",
          "who",
          "still",
          "even",
          "think",
          "there",
          "ive",
          "life",
          "we",
          "its",
          "make",
          "bit",
          "something",
          "much",
          "could",
          "love",
          "going",
          "things",
          "no",
          "dont",
          "than",
          "way",
          "too",
          "day",
          "their",
          "into",
          "back",
          "has",
          "go",
          "which",
          "don"
         ],
         "type": "bar",
         "x": [
          "feel",
          "and",
          "to",
          "the",
          "of",
          "that",
          "feeling",
          "my",
          "in",
          "it",
          "like",
          "was",
          "so",
          "for",
          "im",
          "but",
          "me",
          "have",
          "is",
          "with",
          "this",
          "am",
          "not",
          "about",
          "be",
          "as",
          "on",
          "you",
          "at",
          "just"
         ],
         "y": [
          13973,
          12721,
          11835,
          11808,
          6824,
          6540,
          6461,
          5422,
          4679,
          4087,
          3661,
          3279,
          3211,
          3189,
          3055,
          2948,
          2942,
          2887,
          2808,
          2735,
          2685,
          2608,
          2419,
          2385,
          2297,
          2134,
          2077,
          1962,
          1925,
          1839,
          1819,
          1609,
          1504,
          1474,
          1464,
          1349,
          1240,
          1229,
          1208,
          1201,
          1197,
          1169,
          1143,
          1127,
          1096,
          1091,
          1088,
          1083,
          1083,
          1032,
          1021,
          1007,
          978,
          968,
          968,
          960,
          947,
          920,
          910,
          874,
          865,
          847,
          836,
          815,
          811,
          803,
          802,
          800,
          799,
          797,
          792,
          759,
          746,
          744,
          727,
          723,
          694,
          683,
          673,
          656,
          650,
          646,
          643,
          627,
          620,
          616,
          615,
          610,
          596,
          593,
          593,
          558,
          558,
          545,
          544,
          540,
          536,
          513,
          510,
          509
         ]
        }
       ],
       "layout": {
        "barmode": "group",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Fréquence d’apparition des mots "
        },
        "xaxis": {
         "title": {
          "text": "word rank"
         }
        },
        "yaxis": {
         "title": {
          "text": "word frequency"
         }
        }
       }
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "\n",
    "\n",
    "# freq = subsample(wrank)\n",
    "# r = np.arange(len(freq))\n",
    "\n",
    "trace = go.Bar(\n",
    "                x = x_word.head(30),\n",
    "                y = y_nbr,\n",
    "                name = \"Le score universitaire pour le transfert de connaissances par pays\",\n",
    "                marker = dict(color = 'rgba(255, 87, 51, 0.5)', line = dict(color ='rgb(0,0,0)',width =2.5)),\n",
    "                text = df_up['Word'])\n",
    "\n",
    "layout = go.Layout(barmode = \"group\",\n",
    "                  title = 'Fréquence d’apparition des mots ',\n",
    "                  yaxis = dict(title = 'word frequency'),\n",
    "                  xaxis = dict(title = 'word rank'))\n",
    "fig = go.Figure(data = trace, layout = layout)\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}