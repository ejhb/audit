{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bitbasecondac27713b0d4da4424a3036238a0ac4095",
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------IMPORT----------------------------------------------------------------------------#\n",
    "#dieu\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from time import time\n",
    "# Sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer , TfidfTransformer\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import recall_score , precision_score ,f1_score\n",
    "from sklearn import metrics , svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.naive_bayes import MultinomialNB, CategoricalNB, ComplementNB, BernoulliNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "#vis\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objs as go\n",
    "import plotly as py\n",
    "from plotly.offline import init_notebook_mode, iplot, plot\n",
    "#NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords , wordnet as wn\n",
    "from nltk import wordpunct_tokenize , WordNetLemmatizer ,sent_tokenize ,  word_tokenize\n",
    "from nltk.stem import PorterStemmer , LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------DATAFRAME DISPLAY------------------------------------------------------------------#\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.width', None)\n",
    "# pd.set_option('display.max_colwidth', -1)\n",
    "pd.reset_option(\"^display\") ##Reset display option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------FUNCTIONS--------------------------------------------------------------------#\n",
    "def get_top_n_words(corpus,d,n=None):\n",
    "    vec = CountVectorizer().fit(corpus.str(lower))\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n",
    "    if d == \"up\" :\n",
    "        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "        return words_freq[:n]\n",
    "    elif d == \"down\" :\n",
    "        words_freq=sorted(words_freq, key = lambda x: x[1], reverse=False)\n",
    "        return words_freq[:n]\n",
    "\n",
    "def tokenize(corpus):\n",
    "    #text = ''.join([ch for ch in text if ch not in dataEF[\"Text\"]])\n",
    "    tokens = nltk.word_tokenize()\n",
    "    lemma = WordNetLemmatizer()\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    # tokens = [lemmatizer.lemmatize(token) for token in tokens] \n",
    "    #OR\n",
    "    # tokens = [lemma.lemmatize(lemma.lemmatize(lemma.lemmatize(w,'v'),'n'),'a')for w in tokens]\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "def run_pipes(pipes, splits=10, test_size=0.2, seed=42):  \n",
    "    res = defaultdict(list)\n",
    "    spliter = ShuffleSplit(n_splits=splits, test_size=test_size, random_state=seed)\n",
    "    for idx_train, idx_test in spliter.split(corpus):\n",
    "        for pipe in pipes:\n",
    "            # name of the model\n",
    "            name = \"-\".join([x[0] for x in pipe.steps])\n",
    "            \n",
    "            # extract datasets\n",
    "            X_train = corpus[idx_train]\n",
    "            X_test = corpus[idx_test]\n",
    "            y_train = targets[idx_train]\n",
    "            y_test = targets[idx_test]\n",
    "            \n",
    "            # Learn\n",
    "            start = time()\n",
    "            pipe.fit(X_train, y_train)\n",
    "            fit_time = time() - start\n",
    "            \n",
    "            # predict and save results\n",
    "            y = pipe.predict(X_test)\n",
    "            res[name].append([\n",
    "                fit_time,\n",
    "                f1_score(y_test, y, average = 'micro')\n",
    "            ])\n",
    "    return res\n",
    "\n",
    "def print_table(res):\n",
    "    # Compute mean and std\n",
    "    final = {}\n",
    "    for model in res:\n",
    "        arr = np.array(res[model])\n",
    "        final[model] = {\n",
    "            \"time\" : arr[:, 0].mean().round(2),\n",
    "            \"f1_score\": [arr[:,1].mean().round(5),arr[:,1].std().round(5)]\n",
    "                    }\n",
    "\n",
    "    df = pd.DataFrame.from_dict(final, orient=\"index\").round(3)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "argument of type 'NoneType' is not iterable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4c9478b9e0ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf0_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenized_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf0_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Tokenization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf0_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenized_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf0_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenized_sents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_word_punct\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdf0_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenized_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_sw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf0_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenized_sents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdf0_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenized_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf0_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenized_sents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Stem every word.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3848\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-4c9478b9e0ec>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf0_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenized_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf0_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Tokenization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf0_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenized_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf0_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenized_sents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_word_punct\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdf0_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenized_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_sw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf0_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenized_sents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdf0_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenized_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf0_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenized_sents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Stem every word.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-4c9478b9e0ec>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf0_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenized_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf0_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Tokenization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf0_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenized_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf0_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenized_sents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_word_punct\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mdf0_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenized_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_sw\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf0_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenized_sents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mdf0_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenized_sents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf0_pre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenized_sents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Stem every word.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: argument of type 'NoneType' is not iterable"
     ]
    }
   ],
   "source": [
    "# df0_pre = pd.read_csv('../data/emotion_final.csv')\n",
    "\n",
    "# exclude = set(string.punctuation) # exclude = punctuation strings\n",
    "# stop_word = stopwords.words('english') # we choosing stop words of english dict\n",
    "# stop_word_punct = stop_word.extend(exclude) # we add strings punctions to stop word dict\n",
    "\n",
    "# df0_pre.tokenized_sents = df0_pre.apply(lambda row: word_tokenize(row['Text']), axis=1) # Tokenization\n",
    "# df0_pre.tokenized_sents = df0_pre.tokenized_sents.apply(lambda x: [item for item in x if item not in stop_word_punct])\n",
    "# df0_pre.tokenized_sents = [[lemma.lemmatize(word) for word in each if word not in stop_sw] for each in df0_pre.tokenized_sents] \n",
    "# df0_pre.tokenized_sents = df0_pre.tokenized_sents.apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word.\n",
    "# df0_pre.tokenized_sents = df0_pre.tokenized_sents.apply(lambda x: [porter.stem(y) for y in x]) # Stem every word.\n",
    "# df0_pre.tokenized_sents = df0_pre.tokenized_sents.apply(lambda x: [lancaster.stem(y) for y in x]) # Stem every word.\n",
    "# #delete list in list, **********need to be in one line*********\n",
    "# dz = df0_pre.tokenized_sents\n",
    "# dz = [[' '.join(i)][0] for i in dz]\n",
    "# df0_pre.tokenized_sents = dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'stop_sw' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-95b11250ec2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mexclude\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# exclude = punctuation strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstop_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# we choosing stop words of english dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mstop_word_punct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstop_sw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# we add strings punctions to stop word dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop_sw' is not defined"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('../data/emotion_final.csv')\n",
    "\n",
    "exclude = set(string.punctuation) # exclude = punctuation strings\n",
    "stop_word = stopwords.words('english') # we choosing stop words of english dict\n",
    "stop_word_punct = stop_sw.extend(exclude) # we add strings punctions to stop word dict\n",
    "\n",
    "corpus = np.array(df1[\"Text\"])\n",
    "targets = np.array(df1[\"Emotion\"])\n",
    "targets = np.array([1 if x == \"sadness\" else 2 if x==\"anger\" else 3 if x==\"love\" else 4 if x==\"surprise\" else 5 if x==\"fear\" else 6 for x in targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe0 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('sgd', SGDClassifier()),\n",
    "])\n",
    "pipe1 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('svm', SVC()),\n",
    "])\n",
    "pipe2 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('sgd', SGDClassifier()),\n",
    "])\n",
    "pipe3 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('svm', LinearSVC()),\n",
    "])\n",
    "pipe4 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('svm', SVC()),\n",
    "])\n",
    "pipe5 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('logit', LogisticRegression()),\n",
    "])\n",
    "pipe6 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('mult_nb', MultinomialNB()),\n",
    "])\n",
    "pipe7 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('compl_nb', ComplementNB()),\n",
    "])\n",
    "pipe8 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('bern_nb', BernoulliNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = run_pipes([pipe0, pipe1, pipe2, pipe3, pipe4, pipe5, pipe6, pipe7, pipe8], splits=5)\n",
    "print_table(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe0 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('sgd', SGDClassifier()),\n",
    "])\n",
    "pipe1 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('svm', SVC()),\n",
    "])\n",
    "pipe2 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('sgd', SGDClassifier()),\n",
    "])\n",
    "pipe3 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('svm', LinearSVC()),\n",
    "])\n",
    "pipe4 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('svm', SVC()),\n",
    "])\n",
    "pipe5 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('logit', LogisticRegression()),\n",
    "])\n",
    "pipe6 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('mult_nb', MultinomialNB()),\n",
    "])\n",
    "pipe7 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('compl_nb', ComplementNB()),\n",
    "])\n",
    "pipe8 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('bern_nb', BernoulliNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'run_pipes' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-447cab9ea667>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# run base pipes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_pipes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpipe0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipe1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipe2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipe3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipe4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipe5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipe6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipe7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipe8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'run_pipes' is not defined"
     ]
    }
   ],
   "source": [
    "# run base pipes\n",
    "res = run_pipes([pipe0, pipe1, pipe2, pipe3, pipe4, pipe5, pipe6, pipe7, pipe8], splits=5)\n",
    "print_table(res)\n",
    "\n",
    "# \ttime\tf1_score\n",
    "# vect-sgd\t0.64\t[0.887, 0.005]\n",
    "# vect-svm\t91.26\t[0.787, 0.003]\n",
    "# vect-tfidf-sgd\t0.50\t[0.89, 0.003]\n",
    "# vect-tfidf-svml\t0.59\t[0.891, 0.007]\n",
    "# vect-tfidf-svm\t97.59\t[0.845, 0.006]\n",
    "# vect-tfidf-logit\t4.45\t[0.854, 0.006]\n",
    "# vect-mult_nb\t0.25\t[0.745, 0.005]\n",
    "# vect-compl_nb\t0.24\t[0.877, 0.003]\n",
    "# vect-bern_nb\t0.25\t[0.655, 0.008]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    Emotion  Count\n0     anger   2993\n1      fear   2652\n2     happy   7029\n3      love   1641\n4   sadness   6265\n5  surprise    879\n"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('../data/emotion_final.csv')\n",
    "\n",
    "# print(df1['Emotion'].unique())\n",
    "df_count_emotion = df1.groupby(['Emotion']).size().reset_index(name='Count')\n",
    "print(df_count_emotion) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = set(string.punctuation) # exclude = punctuation strings\n",
    "stop_words = stopwords.words('english') # we choosing stop words of english dict\n",
    "stop_words.extend(exclude) # we add strings punctions to stop word dict\n",
    "lemma = WordNetLemmatizer()\n",
    "stemmer = SnowballStemmer(\"english\") # we choosing the language english for the stemmization \n",
    "porter = PorterStemmer() \n",
    "lancaster=LancasterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                    Text  Emotion\n",
       "0                                  didnt feel humiliated  sadness\n",
       "1      go feeling hopeless damned hopeful around some...  sadness\n",
       "2              im grabbing minute post feel greedy wrong    anger\n",
       "3      ever feeling nostalgic fireplace know still pr...     love\n",
       "4                                        feeling grouchy    anger\n",
       "...                                                  ...      ...\n",
       "21454                         Melissa stared friend dism     fear\n",
       "21455  Successive state election seen governing party...     fear\n",
       "21456                           Vincent irritated dismay     fear\n",
       "21457        Kendall-Hume turned back face dismayed coup     fear\n",
       "21458                                 I dismayed surpris     fear\n",
       "\n",
       "[21459 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Emotion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>didnt feel humiliated</td>\n      <td>sadness</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>go feeling hopeless damned hopeful around some...</td>\n      <td>sadness</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>im grabbing minute post feel greedy wrong</td>\n      <td>anger</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ever feeling nostalgic fireplace know still pr...</td>\n      <td>love</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>feeling grouchy</td>\n      <td>anger</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>21454</th>\n      <td>Melissa stared friend dism</td>\n      <td>fear</td>\n    </tr>\n    <tr>\n      <th>21455</th>\n      <td>Successive state election seen governing party...</td>\n      <td>fear</td>\n    </tr>\n    <tr>\n      <th>21456</th>\n      <td>Vincent irritated dismay</td>\n      <td>fear</td>\n    </tr>\n    <tr>\n      <th>21457</th>\n      <td>Kendall-Hume turned back face dismayed coup</td>\n      <td>fear</td>\n    </tr>\n    <tr>\n      <th>21458</th>\n      <td>I dismayed surpris</td>\n      <td>fear</td>\n    </tr>\n  </tbody>\n</table>\n<p>21459 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "df_pre = pd.read_csv('../data/emotion_final.csv')\n",
    "df_pr_pre['Text'] = df_pre.apply(lambda row: word_tokenize(row['Text']), axis=1) # Tokenization\n",
    "df_pre['Text'] = df_pre['Text'].apply(lambda x: [item for item in x if item not in stop_words]) # Stop wordization :) coucou anne-laure\n",
    "df_pre['Text'] = [[lemma.lemmatize(word) for word in each if word not in stop_sw] for each in df_pre['Text']]  # Lemmization\n",
    "# df_pre['Text'] = df1['Text'].apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word. with snowball('english')\n",
    "# df_pre['Text'] = df1['Text'].apply(lambda x: [porter.stem(y) for y in x]) # Stem every word. with porter\n",
    "# df_pre['Text'] = df1['Text'].apply(lambda x: [lancaster.stem(y) for y in x]) # Stem every word. with lancaster\n",
    "dz = df_pre['Text']\n",
    "dz = [[' '.join(i)][0] for i in dz] \n",
    "df_pre['Text'] = dz\n",
    "df_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                    Text  Emotion\n",
       "0                                       didnt feel humil  sadness\n",
       "1       go feel hopeless damn hop around someon car awak  sadness\n",
       "2                    im grab minut post feel greed wrong    anger\n",
       "3             ev feel nostalg fireplac know stil propert     love\n",
       "4                                            feel grouch    anger\n",
       "...                                                  ...      ...\n",
       "21454                            meliss star friend dism     fear\n",
       "21455  success stat elect seen govern part pummel dis...     fear\n",
       "21456                                  vint irrit dismay     fear\n",
       "21457              kendall-hum turn back fac dismay coup     fear\n",
       "21458                                     i dismay surpr     fear\n",
       "\n",
       "[21459 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Emotion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>didnt feel humil</td>\n      <td>sadness</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>go feel hopeless damn hop around someon car awak</td>\n      <td>sadness</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>im grab minut post feel greed wrong</td>\n      <td>anger</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ev feel nostalg fireplac know stil propert</td>\n      <td>love</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>feel grouch</td>\n      <td>anger</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>21454</th>\n      <td>meliss star friend dism</td>\n      <td>fear</td>\n    </tr>\n    <tr>\n      <th>21455</th>\n      <td>success stat elect seen govern part pummel dis...</td>\n      <td>fear</td>\n    </tr>\n    <tr>\n      <th>21456</th>\n      <td>vint irrit dismay</td>\n      <td>fear</td>\n    </tr>\n    <tr>\n      <th>21457</th>\n      <td>kendall-hum turn back fac dismay coup</td>\n      <td>fear</td>\n    </tr>\n    <tr>\n      <th>21458</th>\n      <td>i dismay surpr</td>\n      <td>fear</td>\n    </tr>\n  </tbody>\n</table>\n<p>21459 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "df1 = pd.read_csv('../data/emotion_final.csv')\n",
    "df1['Text'] = df1.apply(lambda row: word_tokenize(row['Text']), axis=1) # Tokenization\n",
    "df1['Text'] = df1['Text'].apply(lambda x: [item for item in x if item not in stop_sw]) # Stop wordization :) coucou anne-laure\n",
    "# df1['Text'] = [[lemma.lemmatize(word) for word in each if word not in stop_sw] for each in df1['Text']]  # Lemmization\n",
    "# df1['Text'] = df1['Text'].apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word. with snowball('english')\n",
    "df1['Text'] = df1['Text'].apply(lambda x: [porter.stem(y) for y in x]) # Stem every word. with porter\n",
    "df1['Text'] = df1['Text'].apply(lambda x: [lancaster.stem(y) for y in x]) # Stem every word. with lancaster\n",
    "\n",
    "dz = df1['Text']\n",
    "dz = [[' '.join(i)][0] for i in dz] \n",
    "\n",
    "df1['Text'] = dz\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       Emotion                                               Text\n",
       "0      sadness                              didnt feel humiliated\n",
       "1      sadness  go feeling hopeless damned hopeful around some...\n",
       "2        anger          im grabbing minute post feel greedy wrong\n",
       "3         love  ever feeling nostalgic fireplace know still pr...\n",
       "4        anger                                    feeling grouchy\n",
       "...        ...                                                ...\n",
       "21454     fear                         Melissa stared friend dism\n",
       "21455     fear  Successive state election seen governing party...\n",
       "21456     fear                           Vincent irritated dismay\n",
       "21457     fear        Kendall-Hume turned back face dismayed coup\n",
       "21458     fear                                 I dismayed surpris\n",
       "\n",
       "[21459 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Emotion</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>sadness</td>\n      <td>didnt feel humiliated</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sadness</td>\n      <td>go feeling hopeless damned hopeful around some...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>anger</td>\n      <td>im grabbing minute post feel greedy wrong</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>love</td>\n      <td>ever feeling nostalgic fireplace know still pr...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>anger</td>\n      <td>feeling grouchy</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>21454</th>\n      <td>fear</td>\n      <td>Melissa stared friend dism</td>\n    </tr>\n    <tr>\n      <th>21455</th>\n      <td>fear</td>\n      <td>Successive state election seen governing party...</td>\n    </tr>\n    <tr>\n      <th>21456</th>\n      <td>fear</td>\n      <td>Vincent irritated dismay</td>\n    </tr>\n    <tr>\n      <th>21457</th>\n      <td>fear</td>\n      <td>Kendall-Hume turned back face dismayed coup</td>\n    </tr>\n    <tr>\n      <th>21458</th>\n      <td>fear</td>\n      <td>I dismayed surpris</td>\n    </tr>\n  </tbody>\n</table>\n<p>21459 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "data = {'Emotion':df1['Emotion'],'Text':df1['Text']}\n",
    "df28 = pd.DataFrame(data=data)\n",
    "df28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('../data/emotion_final.csv')\n",
    "df1['Text'] = df1.apply(lambda row: word_tokenize(row['Text']), axis=1) # Tokenization\n",
    "df1['Text'] = df1['Text'].apply(lambda x: [item for item in x if item not in stop_sw]) # Stop wordization :) coucou anne-laure\n",
    "# df1['Text'] = [[lemma.lemmatize(word) for word in each if word not in stop_sw] for each in df1['Text']]  # Lemmization\n",
    "df1['Text'] = df1['Text'].apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word. with snowball('english')\n",
    "df1['Text'] = df1['Text'].apply(lambda x: [porter.stem(y) for y in x]) # Stem every word. with porter\n",
    "df1['Text'] = df1['Text'].apply(lambda x: [lancaster.stem(y) for y in x]) # Stem every word. with lancaster\n",
    "\n",
    "dz = df1['Text']\n",
    "dz = [[' '.join(i)][0] for i in dz] \n",
    "\n",
    "df1['Text'] = dz\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         tweet_id   sentiment         author  \\\n",
       "0      1956967341       empty     xoshayzers   \n",
       "1      1956967666     sadness      wannamama   \n",
       "2      1956967696     sadness      coolfunky   \n",
       "3      1956967789  enthusiasm    czareaquino   \n",
       "4      1956968416     neutral      xkilljoyx   \n",
       "...           ...         ...            ...   \n",
       "39995  1753918954     neutral  showMe_Heaven   \n",
       "39996  1753919001        love       drapeaux   \n",
       "39997  1753919005        love       JenniRox   \n",
       "39998  1753919043   happiness       ipdaman1   \n",
       "39999  1753919049        love    Alpharalpha   \n",
       "\n",
       "                                                 content  \\\n",
       "0      @tiffanylue i know  i was listenin to bad habi...   \n",
       "1      Layin n bed with a headache  ughhhh...waitin o...   \n",
       "2                    Funeral ceremony...gloomy friday...   \n",
       "3                   wants to hang out with friends SOON!   \n",
       "4      @dannycastillo We want to trade with someone w...   \n",
       "...                                                  ...   \n",
       "39995                                   @JohnLloydTaylor   \n",
       "39996                     Happy Mothers Day  All my love   \n",
       "39997  Happy Mother's Day to all the mommies out ther...   \n",
       "39998  @niariley WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEE...   \n",
       "39999  @mopedronin bullet train from tokyo    the gf ...   \n",
       "\n",
       "                                         tokenized_sents  \n",
       "0      [tiffanylu, know, listenin, bad, habit, ear, s...  \n",
       "1      [layin, n, bed, headach, ughhhh, ..., waitin, ...  \n",
       "2                [fun, ceremon, ..., gloom, friday, ...]  \n",
       "3                             [want, hang, friend, soon]  \n",
       "4      [dannycastillo, we, want, trad, someon, housto...  \n",
       "...                                                  ...  \n",
       "39995                                    [johnlloydtayl]  \n",
       "39996                         [happ, moth, day, al, lov]  \n",
       "39997  [happ, moth, 's, day, momm, wom, man, long, re...  \n",
       "39998  [niariley, wassup, beaut, follow, me, peep, ou...  \n",
       "39999  [mopedronin, bullet, train, tokyo, gf, visit, ...  \n",
       "\n",
       "[40000 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>sentiment</th>\n      <th>author</th>\n      <th>content</th>\n      <th>tokenized_sents</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1956967341</td>\n      <td>empty</td>\n      <td>xoshayzers</td>\n      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n      <td>[tiffanylu, know, listenin, bad, habit, ear, s...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1956967666</td>\n      <td>sadness</td>\n      <td>wannamama</td>\n      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n      <td>[layin, n, bed, headach, ughhhh, ..., waitin, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1956967696</td>\n      <td>sadness</td>\n      <td>coolfunky</td>\n      <td>Funeral ceremony...gloomy friday...</td>\n      <td>[fun, ceremon, ..., gloom, friday, ...]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1956967789</td>\n      <td>enthusiasm</td>\n      <td>czareaquino</td>\n      <td>wants to hang out with friends SOON!</td>\n      <td>[want, hang, friend, soon]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1956968416</td>\n      <td>neutral</td>\n      <td>xkilljoyx</td>\n      <td>@dannycastillo We want to trade with someone w...</td>\n      <td>[dannycastillo, we, want, trad, someon, housto...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>39995</th>\n      <td>1753918954</td>\n      <td>neutral</td>\n      <td>showMe_Heaven</td>\n      <td>@JohnLloydTaylor</td>\n      <td>[johnlloydtayl]</td>\n    </tr>\n    <tr>\n      <th>39996</th>\n      <td>1753919001</td>\n      <td>love</td>\n      <td>drapeaux</td>\n      <td>Happy Mothers Day  All my love</td>\n      <td>[happ, moth, day, al, lov]</td>\n    </tr>\n    <tr>\n      <th>39997</th>\n      <td>1753919005</td>\n      <td>love</td>\n      <td>JenniRox</td>\n      <td>Happy Mother's Day to all the mommies out ther...</td>\n      <td>[happ, moth, 's, day, momm, wom, man, long, re...</td>\n    </tr>\n    <tr>\n      <th>39998</th>\n      <td>1753919043</td>\n      <td>happiness</td>\n      <td>ipdaman1</td>\n      <td>@niariley WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEE...</td>\n      <td>[niariley, wassup, beaut, follow, me, peep, ou...</td>\n    </tr>\n    <tr>\n      <th>39999</th>\n      <td>1753919049</td>\n      <td>love</td>\n      <td>Alpharalpha</td>\n      <td>@mopedronin bullet train from tokyo    the gf ...</td>\n      <td>[mopedronin, bullet, train, tokyo, gf, visit, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>40000 rows × 5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 124
    }
   ],
   "source": [
    "df2 = pd.read_csv('../data/text_emotion.csv')\n",
    "\n",
    "df2['tokenized_sents'] = df2.apply(lambda row: word_tokenize(row['content']), axis=1) # Tokenization\n",
    "df2['tokenized_sents'] = df2['tokenized_sents'].apply(lambda x: [item for item in x if item not in stop_sw])\n",
    "df2.tokenized_sents = [[lemma.lemmatize(word) for word in each if word not in stop_sw] for each in df2.tokenized_sents] \n",
    "df2['tokenized_sents'] = df2['tokenized_sents'].apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word.\n",
    "df2['tokenized_sents'] = df2['tokenized_sents'].apply(lambda x: [porter.stem(y) for y in x]) # Stem every word.\n",
    "df2['tokenized_sents'] = df2['tokenized_sents'].apply(lambda x: [lancaster.stem(y) for y in x]) # Stem every word.\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         tweet_id   sentiment         author  \\\n",
       "0      1956967341       empty     xoshayzers   \n",
       "1      1956967666     sadness      wannamama   \n",
       "2      1956967696     sadness      coolfunky   \n",
       "3      1956967789  enthusiasm    czareaquino   \n",
       "4      1956968416     neutral      xkilljoyx   \n",
       "...           ...         ...            ...   \n",
       "39995  1753918954     neutral  showMe_Heaven   \n",
       "39996  1753919001        love       drapeaux   \n",
       "39997  1753919005        love       JenniRox   \n",
       "39998  1753919043   happiness       ipdaman1   \n",
       "39999  1753919049        love    Alpharalpha   \n",
       "\n",
       "                                                 content  \\\n",
       "0      @tiffanylue i know  i was listenin to bad habi...   \n",
       "1      Layin n bed with a headache  ughhhh...waitin o...   \n",
       "2                    Funeral ceremony...gloomy friday...   \n",
       "3                   wants to hang out with friends SOON!   \n",
       "4      @dannycastillo We want to trade with someone w...   \n",
       "...                                                  ...   \n",
       "39995                                   @JohnLloydTaylor   \n",
       "39996                     Happy Mothers Day  All my love   \n",
       "39997  Happy Mother's Day to all the mommies out ther...   \n",
       "39998  @niariley WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEE...   \n",
       "39999  @mopedronin bullet train from tokyo    the gf ...   \n",
       "\n",
       "                                         tokenized_sents  \\\n",
       "0      tiffanylu know listenin bad habit ear start fr...   \n",
       "1          layin n bed headach ughhhh ... waitin cal ...   \n",
       "2                       fun ceremon ... gloom friday ...   \n",
       "3                                  want hang friend soon   \n",
       "4      dannycastillo we want trad someon houston tick...   \n",
       "...                                                  ...   \n",
       "39995                                      johnlloydtayl   \n",
       "39996                               happ moth day al lov   \n",
       "39997  happ moth 's day momm wom man long re momm som...   \n",
       "39998  niariley wassup beaut follow me peep out my ne...   \n",
       "39999  mopedronin bullet train tokyo gf visit jap sin...   \n",
       "\n",
       "                                                  simple  \n",
       "0      tiffanylu know listenin bad habit ear start fr...  \n",
       "1          layin n bed headach ughhhh ... waitin cal ...  \n",
       "2                       fun ceremon ... gloom friday ...  \n",
       "3                                  want hang friend soon  \n",
       "4      dannycastillo we want trad someon houston tick...  \n",
       "...                                                  ...  \n",
       "39995                                      johnlloydtayl  \n",
       "39996                               happ moth day al lov  \n",
       "39997  happ moth 's day momm wom man long re momm som...  \n",
       "39998  niariley wassup beaut follow me peep out my ne...  \n",
       "39999  mopedronin bullet train tokyo gf visit jap sin...  \n",
       "\n",
       "[40000 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>sentiment</th>\n      <th>author</th>\n      <th>content</th>\n      <th>tokenized_sents</th>\n      <th>simple</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1956967341</td>\n      <td>empty</td>\n      <td>xoshayzers</td>\n      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n      <td>tiffanylu know listenin bad habit ear start fr...</td>\n      <td>tiffanylu know listenin bad habit ear start fr...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1956967666</td>\n      <td>sadness</td>\n      <td>wannamama</td>\n      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n      <td>layin n bed headach ughhhh ... waitin cal ...</td>\n      <td>layin n bed headach ughhhh ... waitin cal ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1956967696</td>\n      <td>sadness</td>\n      <td>coolfunky</td>\n      <td>Funeral ceremony...gloomy friday...</td>\n      <td>fun ceremon ... gloom friday ...</td>\n      <td>fun ceremon ... gloom friday ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1956967789</td>\n      <td>enthusiasm</td>\n      <td>czareaquino</td>\n      <td>wants to hang out with friends SOON!</td>\n      <td>want hang friend soon</td>\n      <td>want hang friend soon</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1956968416</td>\n      <td>neutral</td>\n      <td>xkilljoyx</td>\n      <td>@dannycastillo We want to trade with someone w...</td>\n      <td>dannycastillo we want trad someon houston tick...</td>\n      <td>dannycastillo we want trad someon houston tick...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>39995</th>\n      <td>1753918954</td>\n      <td>neutral</td>\n      <td>showMe_Heaven</td>\n      <td>@JohnLloydTaylor</td>\n      <td>johnlloydtayl</td>\n      <td>johnlloydtayl</td>\n    </tr>\n    <tr>\n      <th>39996</th>\n      <td>1753919001</td>\n      <td>love</td>\n      <td>drapeaux</td>\n      <td>Happy Mothers Day  All my love</td>\n      <td>happ moth day al lov</td>\n      <td>happ moth day al lov</td>\n    </tr>\n    <tr>\n      <th>39997</th>\n      <td>1753919005</td>\n      <td>love</td>\n      <td>JenniRox</td>\n      <td>Happy Mother's Day to all the mommies out ther...</td>\n      <td>happ moth 's day momm wom man long re momm som...</td>\n      <td>happ moth 's day momm wom man long re momm som...</td>\n    </tr>\n    <tr>\n      <th>39998</th>\n      <td>1753919043</td>\n      <td>happiness</td>\n      <td>ipdaman1</td>\n      <td>@niariley WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEE...</td>\n      <td>niariley wassup beaut follow me peep out my ne...</td>\n      <td>niariley wassup beaut follow me peep out my ne...</td>\n    </tr>\n    <tr>\n      <th>39999</th>\n      <td>1753919049</td>\n      <td>love</td>\n      <td>Alpharalpha</td>\n      <td>@mopedronin bullet train from tokyo    the gf ...</td>\n      <td>mopedronin bullet train tokyo gf visit jap sin...</td>\n      <td>mopedronin bullet train tokyo gf visit jap sin...</td>\n    </tr>\n  </tbody>\n</table>\n<p>40000 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 123
    }
   ],
   "source": [
    "corpus = df2['tokenized_sents']\n",
    "x = corpus\n",
    "x = [[' '.join(i)][0] for i in x]\n",
    "\n",
    "df2['tokenized_sents'] = x\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-- TF-IDF vector shape : (21459, 129522)\n"
     ]
    }
   ],
   "source": [
    "corpus = np.array(df1['Text'])\n",
    "x = corpus\n",
    "\n",
    "# TF-IDF Vectorizer \n",
    "v_tf = TfidfVectorizer(stop_words=stop_sw, tokenizer=tokenize, ngram_range=(1,2))\n",
    "x_tf = v_tf.fit_transform(x)\n",
    "# TF-IDF Model\n",
    "idf_values = dict(zip(v_tf.get_feature_names(), v_tf.idf_))\n",
    "words_name = v_tf.get_feature_names()\n",
    "\n",
    "print(\"-- TF-IDF vector shape :\",x_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-- Count_Vectorizer shape : (21459, 129522)\n"
     ]
    }
   ],
   "source": [
    "corpus = np.array(df1['Text'])\n",
    "x = corpus\n",
    "\n",
    "cv = CountVectorizer(stop_words=stop_sw, tokenizer=tokenize, ngram_range=(1,2))\n",
    "x_cv = cv.fit_transform(x) ## Count Vectorizer Model\n",
    "print(\"-- Count_Vectorizer shape :\",x_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train pack with TF-IDF Vectorizer \n",
    "x_tf_train, x_tf_test, y_train, y_test = train_test_split(x_tf, y, test_size=0.20, random_state=0)\n",
    "print('-- Training pack TF-IDF:',\"\\n\")\n",
    "print('-- x_tf_train rows :',np.size(x_tf_train))\n",
    "print('-- x_tf_test rows :',np.size(x_tf_test))\n",
    "print('-- ratio in % :', round(np.size(x_tf_test)/(np.size(x_tf_train)+np.size(x_tf_test))*100,2),\"\\n\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-- TF-IDF vector shape : (40000, 245784)\n"
     ]
    }
   ],
   "source": [
    "corpus = np.array(df2['content'])\n",
    "x = corpus\n",
    "\n",
    "# TF-IDF Vectorizer \n",
    "v_tf = TfidfVectorizer(stop_words=stop_sw, tokenizer=tokenize, ngram_range=(1,2))\n",
    "x_tf = v_tf.fit_transform(x)\n",
    "# TF-IDF Model\n",
    "idf_values = dict(zip(v_tf.get_feature_names(), v_tf.idf_))\n",
    "words_name = v_tf.get_feature_names()\n",
    "\n",
    "print(\"-- TF-IDF vector shape :\",x_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['@', 'tiffanylu', 'i', 'know', 'i', 'be', 'listenin', 'to', 'bad', 'habit', 'earli', 'and', 'i', 'start', 'freakin', 'at', 'his', 'part', '=', '[']\n",
      "-- Count_Vectorizer shape : (40000, 245784)\n"
     ]
    }
   ],
   "source": [
    "corpus = np.array(df2['content'])\n",
    "x = corpus\n",
    "\n",
    "cv = CountVectorizer(stop_words=stop_sw, tokenizer=tokenize, ngram_range=(1,2))\n",
    "x_cv = cv.fit_transform(x) ## Count Vectorizer Model\n",
    "print(\"-- Count_Vectorizer shape :\",x_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     Word  Number of times\n",
       "0    feel            13973\n",
       "1     and            12721\n",
       "2      to            11835\n",
       "3     the            11808\n",
       "4      of             6824\n",
       "..    ...              ...\n",
       "95   back              540\n",
       "96    has              536\n",
       "97     go              513\n",
       "98  which              510\n",
       "99    don              509\n",
       "\n",
       "[100 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Word</th>\n      <th>Number of times</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>feel</td>\n      <td>13973</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>and</td>\n      <td>12721</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>to</td>\n      <td>11835</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>the</td>\n      <td>11808</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>of</td>\n      <td>6824</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>back</td>\n      <td>540</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>has</td>\n      <td>536</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>go</td>\n      <td>513</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>which</td>\n      <td>510</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>don</td>\n      <td>509</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "df1 = pd.read_csv('../data/emotion_final.csv')\n",
    "\n",
    "x = df1.Text\n",
    "y = df1.Emotion\n",
    "\n",
    "freq_top = get_top_n_words(x,\"up\",100)\n",
    "\n",
    "\n",
    "df_up = pd.DataFrame(freq_top, columns =['Word','Number of times'])\n",
    "y_nbr = df_up['Number of times']\n",
    "x_word = df_up['Word']\n",
    "# df_down = pd.DataFrame(freq_down, columns =['Word','Number of times'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "marker": {
          "color": "rgba(255, 87, 51, 0.5)",
          "line": {
           "color": "rgb(0,0,0)",
           "width": 2.5
          }
         },
         "name": "Le score universitaire pour le transfert de connaissances par pays",
         "text": [
          "feel",
          "and",
          "to",
          "the",
          "of",
          "that",
          "feeling",
          "my",
          "in",
          "it",
          "like",
          "was",
          "so",
          "for",
          "im",
          "but",
          "me",
          "have",
          "is",
          "with",
          "this",
          "am",
          "not",
          "about",
          "be",
          "as",
          "on",
          "you",
          "at",
          "just",
          "when",
          "or",
          "all",
          "more",
          "because",
          "do",
          "can",
          "up",
          "he",
          "really",
          "by",
          "are",
          "very",
          "been",
          "if",
          "know",
          "had",
          "out",
          "her",
          "time",
          "what",
          "myself",
          "how",
          "from",
          "they",
          "little",
          "get",
          "now",
          "will",
          "being",
          "people",
          "would",
          "them",
          "she",
          "want",
          "some",
          "one",
          "an",
          "his",
          "him",
          "who",
          "still",
          "even",
          "think",
          "there",
          "ive",
          "life",
          "we",
          "its",
          "make",
          "bit",
          "something",
          "much",
          "could",
          "love",
          "going",
          "things",
          "no",
          "dont",
          "than",
          "way",
          "too",
          "day",
          "their",
          "into",
          "back",
          "has",
          "go",
          "which",
          "don"
         ],
         "type": "bar",
         "x": [
          "feel",
          "and",
          "to",
          "the",
          "of",
          "that",
          "feeling",
          "my",
          "in",
          "it",
          "like",
          "was",
          "so",
          "for",
          "im",
          "but",
          "me",
          "have",
          "is",
          "with",
          "this",
          "am",
          "not",
          "about",
          "be",
          "as",
          "on",
          "you",
          "at",
          "just"
         ],
         "y": [
          13973,
          12721,
          11835,
          11808,
          6824,
          6540,
          6461,
          5422,
          4679,
          4087,
          3661,
          3279,
          3211,
          3189,
          3055,
          2948,
          2942,
          2887,
          2808,
          2735,
          2685,
          2608,
          2419,
          2385,
          2297,
          2134,
          2077,
          1962,
          1925,
          1839,
          1819,
          1609,
          1504,
          1474,
          1464,
          1349,
          1240,
          1229,
          1208,
          1201,
          1197,
          1169,
          1143,
          1127,
          1096,
          1091,
          1088,
          1083,
          1083,
          1032,
          1021,
          1007,
          978,
          968,
          968,
          960,
          947,
          920,
          910,
          874,
          865,
          847,
          836,
          815,
          811,
          803,
          802,
          800,
          799,
          797,
          792,
          759,
          746,
          744,
          727,
          723,
          694,
          683,
          673,
          656,
          650,
          646,
          643,
          627,
          620,
          616,
          615,
          610,
          596,
          593,
          593,
          558,
          558,
          545,
          544,
          540,
          536,
          513,
          510,
          509
         ]
        }
       ],
       "layout": {
        "barmode": "group",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Fréquence d’apparition des mots "
        },
        "xaxis": {
         "title": {
          "text": "word rank"
         }
        },
        "yaxis": {
         "title": {
          "text": "word frequency"
         }
        }
       }
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "\n",
    "\n",
    "# freq = subsample(wrank)\n",
    "# r = np.arange(len(freq))\n",
    "\n",
    "trace = go.Bar(\n",
    "                x = x_word.head(30),\n",
    "                y = y_nbr,\n",
    "                name = \"Le score universitaire pour le transfert de connaissances par pays\",\n",
    "                marker = dict(color = 'rgba(255, 87, 51, 0.5)', line = dict(color ='rgb(0,0,0)',width =2.5)),\n",
    "                text = df_up['Word'])\n",
    "\n",
    "layout = go.Layout(barmode = \"group\",\n",
    "                  title = 'Fréquence d’apparition des mots ',\n",
    "                  yaxis = dict(title = 'word frequency'),\n",
    "                  xaxis = dict(title = 'word rank'))\n",
    "fig = go.Figure(data = trace, layout = layout)\n",
    "iplot(fig)"
   ]
  }
 ]
}